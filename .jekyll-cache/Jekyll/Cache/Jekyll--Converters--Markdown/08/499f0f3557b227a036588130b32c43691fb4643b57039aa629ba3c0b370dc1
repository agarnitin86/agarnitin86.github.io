I"¡"<p><strong><em>Anaphora resolution</em></strong> <em>can be defined as the task of resolving pronouns like</em> <strong><em>he, she, you, me, I, we, us, this, them, that, etc</em></strong> <em>to their matching entity in given context.</em> Context here can be same sentence, paragraph or a larger piece of text. Entity can be a named entity like name of person, place, organization, etc, or in more complex settings it can be noun phrase, verb phrase, sentence, or even whole paragraph. It may not be the final task of NLP problems, but, it is an important task for many high level NLP tasks such as document summarization, question answering, information extraction, aspect based sentiment analysis, etc.</p>

<p>For example: ‚ÄúI have been using <strong>¬´brand_name¬ª</strong> since last 4 years. <strong>Their</strong> products are really good.‚Äù
To attribute the sentiment of second sentence to ¬´brand_name¬ª we will need to resolve <strong>their</strong>, which refers to ¬´brand_name¬ª</p>

<p>Primary types<sup>[1]</sup>:</p>

<ul>
  <li>Pronominal: This is the most common type where a referent is referred by a pronoun.
Example: ‚ÄúJohn found the love of his life‚Äù where ‚Äòhis‚Äô refers to ‚ÄòJohn‚Äô.</li>
  <li>Definite noun phrase: The antecedent is referred by a phrase of the form ‚Äú&lt;the&gt; &lt;noun phrase&gt;‚Äù.
Continued example: ‚ÄúThe relationship did not last long‚Äù, where ‚ÄòThe relationship‚Äô refers
to ‚Äòthe love‚Äô in the preceding sentence.</li>
  <li>Quantifier/Ordinal: The anphor is a quantifier such as ‚Äòone‚Äô or an ordinal such as ‚Äòfirst‚Äô.
Continued Example: ‚ÄúHe started a new one‚Äù where ‚Äòone‚Äô refers to ‚ÄòThe relationship‚Äô
(effectively meaning ‚Äòa relationship‚Äô).</li>
</ul>

<p><strong>Cataphora</strong> : when anaphora <em>precedes</em> the antecedent</p>

<p>For eg.,
Because <strong><em>she</em></strong> was going to the departmental store, <strong><em>Mary</em></strong> was asked to pick up the vegetables.</p>

<p><strong>BERT (Bidirectional Encoder Representations from Transformers)<sup>[2]</sup></strong> is one of latest developments in the field of NLP by Google.
The <a href="https://arxiv.org/pdf/1810.04805.pdf">paper</a> presents two model sizes for BERT:</p>

<ul>
  <li><a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip">BERT-Large, Uncased (Whole Word Masking)</a>: 24-layer, 1024-hidden, 16-heads, 340M parameters</li>
  <li><a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip">BERT-Large, Cased (Whole Word Masking)</a>: 24-layer, 1024-hidden, 16-heads, 340M parameters</li>
</ul>

<p>BERT is the best example of Transfer Learning where we train a general-purpose model on a large text corpus and use that model to solve different NLP tasks. BERT is the first unsupervised, deeply bidirectional system for pre-training NLP. Pre-trained models can be either context-free or contextual. Models like word2vec &amp; GloVe are Context-free models. These models generate a single ‚Äúword embedding‚Äù representation for each word in the vocabulary. But, based on the context in which a word is used it might have a different meaning &amp; hence a different representation, i.e., more than one representation of same word is possible, which is handled by Contextual models.</p>

<p>BERT is a contextual model &amp; takes learnings from techniques like Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. Major enhancement comes with being <em>Deeply Bidirectional</em>, which means, while computing the representation of a word, it takes into account both left &amp; right context in a ‚Äúdeep‚Äù manner.</p>

<p><strong>Problem statement</strong></p>

<p>Given the document, anaphora, and two antecedents or precedents, resolve, which of the two antecedents/precedents does the anaphora refer to.</p>

<p><strong>Solution approach</strong>
Here, we try to use BERT to solve the above mentioned probelm. The problem is formulated as a binary classification problem with two antecedents/precedents being the two classes.</p>

<p>Features used:</p>
<ul>
  <li>Anaphora</li>
  <li>First antecedent or precedent</li>
  <li>Second antecedent or precedent</li>
  <li>Preceeding n words from anaphora</li>
  <li>Preceeding n words from first antecedent or precedent</li>
  <li>Preceeding n words from second antecedent or precedent</li>
  <li>Head word for anaphora</li>
  <li>Head word for first antecedent or precedent</li>
  <li>Head word for second antecedent or precedent</li>
</ul>

<p>We run forward propogation through BERT on our data and extract the output of last layer. This output is the embeddings that we use for creating our features. Once all the feature words/expression are extracted, we extract BERT embeddings for these features. These embeddings are then concatenated &amp; used as features to train the Multi-layer perceptron model.</p>

<p><strong>Steps in detail</strong></p>

<p>To start with you must be having development &amp; test sets. Development set will be used for training the model &amp; test set will be used for final score.</p>

<ol>
  <li>Create input dataset for BERT by setting ‚Äúcreate_bert_input=True‚Äù
This step will create data in format neeeded by BERT &amp; save it to a text file.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from coreference import CoreferenceResolution
CoreferenceResolution.create_bert_input(development_data, 'dev')
</code></pre></div>    </div>
  </li>
  <li>input_<data_type>.txt file will be generated in temp folder of project</data_type></li>
  <li>Use the input file to train BERT embeddings.</li>
  <li>Do the above steps for all the datasets (train/test)</li>
  <li>If you already have BERT embeddings, set create_bert_input=False &amp; run the script.</li>
  <li>Once you have the embeddings, extract relevant features. Currently we extract above mentioned features from the text. This step has scope for improvements.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>development_data['feature_words'] = development_data.apply(CoreferenceResolution.extract_features, axis=1)
test_data['feature_words'] = test_data.apply(CoreferenceResolution.extract_features, axis=1)
</code></pre></div>    </div>
  </li>
  <li>Features that we have created are words/phrases, we need to convert them to numerical format using embeddings. So, extract embeddings for these features using:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>feature_em_dev   = CoreferenceResolution.extract_bert_embedding_for_word(development_data, 'dev')
feature_em_test  = CoreferenceResolution.extract_bert_embedding_for_word(test_data, 'test')
</code></pre></div>    </div>
  </li>
  <li>Concatenate embeddings to create features
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dev_emb_all = CoreferenceResolution.merge_all_features(development_emb, feature_em_dev)
test_emb_all = CoreferenceResolution.merge_all_features(test_emb, feature_em_test)
</code></pre></div>    </div>
  </li>
  <li>
    <p>We can add additional features to the dataframe at this step.</p>
  </li>
  <li>Next we train the classifier using the above created features. We used multi-layered perceptrons for training the model. Again, this piece can be modified to try out other classifiers/networks.</li>
</ol>

<p>The piece of work described here is still under development and we are making improvements to achieve better accuracy. More of it will be described in part-2 of this series.</p>

<p><strong>References</strong></p>

<p>[1] https://nlp.stanford.edu/courses/cs224n/2003/fp/iqsayed/project_report.pdf</p>

<p>[2] https://github.com/google-research/bert</p>

<p>[3] https://www.kaggle.com/c/gendered-pronoun-resolution</p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//agarnitin86-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

:ET
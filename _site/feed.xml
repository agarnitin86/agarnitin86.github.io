<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science &amp; Machine Learning Posts</title>
    <description>This site is designed to contribute to the growing deep learning community. Moslyt focuses on application of deep learning on unstructured data.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 04 Jun 2022 18:28:49 +0530</pubDate>
    <lastBuildDate>Sat, 04 Jun 2022 18:28:49 +0530</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>Data Science Interview Questions</title>
        <description>&lt;h1 id=&quot;data-science-interview-preparation&quot;&gt;Data-Science-Interview-Preparation&lt;/h1&gt;
&lt;p&gt;ISLR = Introduction to Statistical Learning&lt;br /&gt;
ESLR = Elements of Statistical Learning&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/agarnitin86/Data-Science-Interview-Preparation/blob/main/classification.md&quot;&gt;clickme&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;generative-vs-discriminative-models&quot;&gt;Generative vs Discriminative Models&lt;/h2&gt;
&lt;h2 id=&quot;difference-bw-generative--discriminative-models&quot;&gt;Difference b/w generative &amp;amp; discriminative models&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.unite.ai/generative-vs-discriminative-machine-learning-models/&quot;&gt;Generative vs. Discriminative Machine Learning Models - Unite.AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm&quot;&gt;machine learning - What is the difference between a generative and a discriminative algorithm? - Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Generative&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Discriminative&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generative models aim to capture the actual distribution of the classes in the dataset.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Discriminative models model the decision boundary for the dataset classes.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generative models predict the joint probability distribution – p(x,y) – utilizing &lt;a href=&quot;https://www.unite.ai/what-is-bayes-theorem/&quot;&gt;Bayes Theorem&lt;/a&gt;.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Discriminative models learn the conditional probability – p(y|x).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generative models are computationally expensive compared to discriminative models.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Discriminative models are computationally cheap compared to generative models.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generative models are useful for unsupervised machine learning tasks.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Discriminative models are useful for supervised machine learning tasks.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generative models are impacted by the presence of outliers more than discriminative models.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Discriminative models have the advantage of being more robust to outliers, unlike the generative models.Discriminative models are more robust to outliers compared to generative models.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;E.g. Linear Discriminant Analysis, HMM, Bayesian Networks&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;E.g. SVM, Logistic regression, Decision Trees, Random Forests&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;parametric--non-parametric-models&quot;&gt;Parametric &amp;amp; Non-Parametric models&lt;/h1&gt;
&lt;h2 id=&quot;difference-bw-parametric--non-parametric-models&quot;&gt;Difference b/w parametric &amp;amp; non-parametric models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-21]&lt;/em&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Parametric methods involve a two-step model-based approach.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, we make an assumption about the functional form, or shape of f. For example, one very simple assumption is that f is linear in X:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?f(x)=\beta_0&amp;plus;\beta_1x_1&amp;plus;\beta_2x_2&amp;plus;\cdots&amp;plus;\beta_px_p\cdots\cdots(2.4)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a linear model, which will be discussed extensively in Chapter 3. Once we have assumed that f is linear, the problem of estimating f is greatly simplified. Instead of having to estimate an entirely arbitrary p-dimensional function f(X), one only needs to estimate the p+1 coefficients &lt;img src=&quot;https://latex.codecogs.com/svg.image?\beta_0,&amp;space;\beta_1x_1,&amp;space;\beta_2x_2,&amp;space;\cdots,&amp;space;\beta_px_p&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model (2.4), we need to estimate the parameters &lt;img src=&quot;https://latex.codecogs.com/svg.image?\beta_0,&amp;space;\beta_1x_1,&amp;space;\beta_2x_2,&amp;space;\cdots,&amp;space;\beta_px_p&quot; /&gt;. That is, we want to find values of these parameters such that,
&lt;img src=&quot;https://latex.codecogs.com/svg.image?Y\approx\beta_0&amp;plus;\beta_1x_1&amp;plus;\beta_2x_2&amp;plus;\cdots&amp;plus;\beta_px_p&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Non-parametric methods do not make explicit assumptions about the functional form of &lt;em&gt;f&lt;/em&gt;. Instead, they seek an estimate of &lt;em&gt;f&lt;/em&gt; that gets as close to the data points as possible without being too rough or wiggly.&lt;/p&gt;

&lt;p&gt;Some examples of parametric and non-parametric models:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Parametric Models&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Non-parametric Models&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linear regression&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;KNN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Logistic regression&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Decision Trees, Random Forests&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;advantagesdisadvantages-of-parametricnon-parametric-models&quot;&gt;Advantages/disadvantages of Parametric/Non-Parametric models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-23]&lt;/em&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Non-parametric approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for &lt;em&gt;f&lt;/em&gt;, they have the potential to accurately fit a wider range of possible shapes for f. Any parametric approach brings with it the possibility that the functional form used to estimate f is very different from the true f, in which case the resulting model will not fit the data well.&lt;/p&gt;

&lt;p&gt;In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of f is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f.&lt;/p&gt;
&lt;h1 id=&quot;bias--variance&quot;&gt;Bias &amp;amp; Variance&lt;/h1&gt;
&lt;h2 id=&quot;what-do-we-mean-by-the-variance-and-bias-of-a-statistical-learning-method&quot;&gt;What do we mean by the variance and bias of a statistical learning method?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt; refers to the amount by which &lt;em&gt;f&lt;/em&gt; would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different &lt;em&gt;f&lt;/em&gt; . But ideally the estimate for &lt;em&gt;f&lt;/em&gt; should not vary too much between training sets. However, if a method has high variance, then small changes in the training data can result in large changes in &lt;em&gt;f&lt;/em&gt;. In general, more flexible statistical methods have higher variance.&lt;/p&gt;

&lt;p&gt;On the other hand,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between &lt;em&gt;Y&lt;/em&gt; and &lt;img src=&quot;https://latex.codecogs.com/svg.image?X_1,X_2,\cdots,X_p&quot; /&gt;. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of &lt;em&gt;f&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-to-handle-data-imbalance&quot;&gt;How to handle data imbalance:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Under sampling/Oversampling&lt;/li&gt;
  &lt;li&gt;SMOTE&lt;/li&gt;
  &lt;li&gt;Better evaluation metric – like Lift, ROC curves, PR Curves&lt;/li&gt;
  &lt;li&gt;Cost sensitive learning : &lt;a href=&quot;https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/&quot;&gt;Cost-Sensitive Learning for Imbalanced Classification (machinelearningmastery.com)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Class weight balancing : &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/&quot;&gt;How To Dealing With Imbalanced Classes in Machine Learning (analyticsvidhya.com)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Weighted loss function: &lt;a href=&quot;https://medium.com/gumgum-tech/handling-class-imbalance-by-introducing-sample-weighting-in-the-loss-function-3bdebd8203b4&quot;&gt;Handling Class Imbalance by Introducing Sample Weighting in the Loss Function | by Ishan Shrivastava | GumGum Tech Blog | Medium&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;One Class SVM&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;regression&quot;&gt;Regression&lt;/h1&gt;
&lt;h2 id=&quot;difference-bw-tss-total-sum-of-squares--rss-residual-sum-of-square&quot;&gt;Difference b/w TSS (Total Sum of Squares) &amp;amp; RSS (Residual Sum of Square)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?RSS=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2\cdots\cdots(3.16)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?TSS=\sum{(y_i-\bar{y})^2}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TSS&lt;/strong&gt; measures the total variance in the response &lt;strong&gt;Y&lt;/strong&gt; and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, &lt;strong&gt;RSS&lt;/strong&gt; measures the amount of variability that is left unexplained after performing the regression. Hence, TSS-RSS measures the amount of variability in the response that is explained (or removed) by performing the regression&lt;/p&gt;
&lt;h2 id=&quot;difference-bw-r2-and-adjusted-r2&quot;&gt;Difference b/w R2 and Adjusted R2&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-212]&lt;/em&gt;:&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;R2&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Adjusted R2&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?R^2=\frac{TSS-RSS}{TSS}=\frac{1-RSS}{TSS}&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?{Adjusted&amp;space;R^2}=1-\frac{RSS/{(n-d-1)}}{TSS/{(n-1)}}&quot; /&gt;, where n = number of data points, d = model with d variables&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?R^2&quot; /&gt; will increase as more variables are added to the model.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Maximizing the &lt;img src=&quot;https://latex.codecogs.com/svg.image?Adjusted&amp;space;R^2&quot; /&gt; is equivalent to minimizing &lt;img src=&quot;https://latex.codecogs.com/svg.image?{RSS}/{(n-d-1)}&quot; /&gt;. While RSS always decreases as the number of variables in the model increases, RSS/(n-d-1) may increase or decrease, due to the presence of d in the denominator.The intuition behind the &lt;img src=&quot;https://latex.codecogs.com/svg.image?Adjusted&amp;space;R^2&quot; /&gt; is that, once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS. Since adding noise variables leads to an increase in d, such variables will lead to an increase in RSS/(n-d-1), and consequently a decrease in the &lt;img src=&quot;https://latex.codecogs.com/svg.image?Adjusted&amp;space;R^2&quot; /&gt;. Therefore, in theory, the model with the largest &lt;img src=&quot;https://latex.codecogs.com/svg.image?Adjusted&amp;space;R^2&quot; /&gt; will have only correct variables and no noise variables.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;lr-assumptions--verification&quot;&gt;LR-Assumptions &amp;amp; Verification&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/&quot;&gt;7 Classical Assumptions of Ordinary Least Squares (OLS) Linear Regression - Statistics By Jim&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/verifying-the-assumptions-of-linear-regression-in-python-and-r-f4cd2907d4c0#:~:text=Verifying%20the%20Assumptions%20of%20Linear%20Regression%20in%20Python,the%20context%20of%20linear%20regression.%20More%20items...%20&quot;&gt;Verifying the Assumptions of Linear Regression in Python and R | by Eryk Lewinson | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Following are the assumptions:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Random error &lt;img src=&quot;https://latex.codecogs.com/svg.image?\varepsilon&quot; /&gt; has &lt;img src=&quot;https://latex.codecogs.com/svg.image?E(\varepsilon)=0&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?\varepsilon&quot; /&gt; is independent of X&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;[Source: ISLR Page-86]&lt;/strong&gt;: Two of the most important assumptions state that the relationship between the predictors and response are additive and linear. The additive assumption means that the effect of changes in a predictor &lt;img src=&quot;https://latex.codecogs.com/svg.image?X_j&quot; /&gt; on the response Y is independent of the values of the other predictors. The linear assumption states that the change in the response Y due to a one-unit change in &lt;img src=&quot;https://latex.codecogs.com/svg.image?X_j&quot; /&gt; is constant, regardless of the value of &lt;img src=&quot;https://latex.codecogs.com/svg.image?X_j&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;[Source: ISLR Page-93]&lt;/strong&gt;: An important assumption of the linear regression model is that the error terms, ε1,ε2, …,εn, are uncorrelated.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;[Source: ISLR Page-95]&lt;/strong&gt;: Error terms have a constant variance, &lt;img src=&quot;https://latex.codecogs.com/svg.image?Var(\varepsilon_i)=\sigma^2&quot; /&gt;. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in the residual plot.
    &lt;h2 id=&quot;implication-of-the-assumption-that-errors-are-independent--identically-distributed&quot;&gt;Implication of the assumption that errors are independent &amp;amp; identically distributed&lt;/h2&gt;
    &lt;p&gt;Because of this assumption, we average squared errors uniformly in our Expected Prediction error criterion. If the errors were dependent, then, weightage of each error might have been different in the error function.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;difference-bw-collineraity--multi-collinearity&quot;&gt;Difference b/w Collineraity &amp;amp; Multi Collinearity&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/254871/what-is-collinearity-and-how-does-it-differ-from-multicollinearity&quot;&gt;Terminology - What is collinearity and how does it differ from multicollinearity? - Cross Validated (stackexchange.com)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://quantifyinghealth.com/correlation-collinearity-multicollinearity/&quot;&gt;Correlation vs Collinearity vs Multicollinearity – Quantifying Health&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Collinearity&lt;/strong&gt; is a linear association between two explanatory variables. Multicollinearity in a multiple regression model are highly linearly related associations between two or more explanatory variables.&lt;/p&gt;

&lt;p&gt;In case of perfect &lt;strong&gt;multicollinearity&lt;/strong&gt;, the design matrix X has less than full rank, and therefore the moment matrix &lt;img src=&quot;https://latex.codecogs.com/svg.image?X^TX&quot; /&gt; cannot be matrix inverted. Under these circumstances, for a general linear model &lt;img src=&quot;https://latex.codecogs.com/svg.image?y=x\beta+\epsilon&quot; /&gt;, the ordinary least-squares estimator &lt;img src=&quot;https://latex.codecogs.com/svg.image?\beta_{OLS}={(X^TX)}^{-1}X^Ty&quot; /&gt; does not exist.&lt;/p&gt;

&lt;h2 id=&quot;how-to-assess-multicollinearity&quot;&gt;How to assess Multicollinearity?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-101]&lt;/em&gt;&lt;/strong&gt; Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is variance inflation factor, the ratio of the variance of βj when fitting the full model divided by the variance of βj if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity&lt;/p&gt;

&lt;h2 id=&quot;how-to-assess-the-quality-of-linear-regression-model&quot;&gt;How to assess the quality of Linear Regression model?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-68]&lt;/em&gt;&lt;/strong&gt;. The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the &lt;img src=&quot;https://latex.codecogs.com/svg.image?R^2&quot; /&gt; statistic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.001.png&quot; alt=&quot;A picture containing text, clock, watch Description automatically generated&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/RSS.png&quot; alt=&quot;A picture containing chart Description automatically generated&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The RSE provides an absolute measure of lack of fit of the model (3.5) to the data. But since it is measured in the units of Y, it is not always clear what constitutes a good RSE. The &lt;img src=&quot;https://latex.codecogs.com/svg.image?R^2&quot; /&gt; statistic provides an alternative measure of fit. It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of Y.&lt;/p&gt;

&lt;p&gt;To calculate &lt;img src=&quot;https://latex.codecogs.com/svg.image?R^2&quot; /&gt;, we use the formula,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.002.png&quot; alt=&quot;Text Description automatically generated&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?TSS=\sum{(y_i-\bar{y})^2}&quot; /&gt; is the total sum of squares, and RSS is defined in (3.16). TSS measures the total variance in the response Y, and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS − RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and &lt;img src=&quot;https://latex.codecogs.com/svg.image?R^2&quot; /&gt; measures the proportion of variability in Y that can be explained using X.&lt;/p&gt;
&lt;h2 id=&quot;regression-approaches-in-order-of-linearity&quot;&gt;Regression approaches in order of linearity&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-266]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Regression Approach&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Explanation&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linear Regression&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Polynomial Regression&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Step Functions&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Step functions cut the range of a variable into K distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Regression Splines&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Regression splines are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of X into K distinct regions. Within each region, a polynomial function is fit to the data.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Smoothing Splines&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Smoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Local Regression&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Local regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Generalized Additive Models&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generalized additive models allow us to extend the methods above to deal with multiple predictors.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;approaches-for-subset-selection&quot;&gt;Approaches for Subset selection&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ESLR Page-57]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Best subset selection&lt;/li&gt;
  &lt;li&gt;Forward stepwise selection&lt;/li&gt;
  &lt;li&gt;Backward stepwise selection&lt;/li&gt;
  &lt;li&gt;Hybrid (Forward + Backward stepwise) selection&lt;/li&gt;
  &lt;li&gt;Forward stagewise regression&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Best subset&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;1. For each k ∈ {0, 1, 2, . . . , p}, where p is #predictors, the subset of size k that gives smallest residual sum of squares.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;2. Typically, we choose the smallest model that minimizes an estimate of the expected prediction error.&amp;lt;/p&amp;gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Forward stepwise&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;1. Is a greedy algorithm &amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;2. Starts with the intercept, and,&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;3. then sequentially adds into the model the predictor that most improves the fit. With many candidate predictors, this might seem like a lot of computation; however, clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate &amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;4. for large p, we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence(even when p ≫ N ).&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;5. forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias than best subset&amp;lt;/p&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Backward stepwise&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;1. starts with the full model, then,&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;2. Sequentially deletes the predictor that has the least impact on the fit. &amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;3. The candidate for dropping is the variable with the smallest Z-score. Backward selection can only be used when N &amp;gt; p, while forward stepwise can always be used.&amp;lt;/p&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Hybrid selection&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;consider both forward and backward moves at each step, and select&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;the “best” of the two.&amp;lt;/p&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Forward stagewise&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;1. more constrained than forward stepwise regression.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;2. Starts like forward-stepwise regression, with an intercept equal to ȳ, and centered predictors with coefficients initially all 0.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;3. At each step, the algorithm identifies the variable most correlated with the current residual. &amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;4. It then computes the simple linear regression coefficient of the residual on this chosen variable, and then adds it to the current coefficient for that variable. This is continued till none of the variables have correlation with the residuals—i.e. the least-squares fit when N &amp;gt; p.&amp;lt;/p&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;difference-between-ridge--lasso-regression&quot;&gt;Difference between Ridge &amp;amp; Lasso regression.&lt;/h2&gt;

&lt;p&gt;Ridge regression:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.011.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lasso Regression:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.012.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Ridge Regression&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Lasso Regression&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;The shrinkage penalty is applied to &lt;img src=&quot;https://latex.codecogs.com/svg.image?\beta_1\cdots\beta_p&quot; /&gt; , but not to the intercept &lt;img src=&quot;https://latex.codecogs.com/svg.image?\beta_0&quot; /&gt;. We do not want to shrink the intercept, which is simply a measure of the mean value of the response when &lt;img src=&quot;https://latex.codecogs.com/svg.image?x_{i1}=x_{i2}=\cdots=x_{ip}=0&quot; /&gt;. (More explanation in ESLR pg. 64)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;It is best to apply ridge regression after&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;standardizing the predictors, using the formula&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&lt;img src=&quot;image/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.013.png&quot; alt=&quot;&quot; /&gt;&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Because, the ridge solutions are not equivariant under scaling of the inputs.&amp;lt;/p&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Ridge regression will include all p predictors in the final model. The penalty &lt;img src=&quot;https://latex.codecogs.com/svg.image?\lambda.\Sigma{\beta_j}^2&quot; /&gt; in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless &lt;img src=&quot;https://latex.codecogs.com/svg.image?\lambda=\infty&quot; /&gt;). This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;L1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Uses l2 penalty&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Uses l1 penalty&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;The l2 norm of a coefficient vector β is given by &lt;img src=&quot;https://latex.codecogs.com/svg.image?||\beta||_2=\Sigma{\beta_j}^2&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;The l1 norm of a coefficient vector β is given by &lt;img src=&quot;https://latex.codecogs.com/svg.image?||\beta||_1=\Sigma|\beta_j|&quot; /&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Will include all p predictors in the final model&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Performs variable selection by setting coefficients of some to the variables to 0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Does not yield sparse models&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;lasso yields sparse models&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;It produces less interpretable models that involve all the predictors.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;it produces simpler and more interpretable models that involve only a subset of the predictors.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;In the case of orthonormal inputs, the ridge estimates are just a scaled version of the least squares estimates, that is, β̂ ridge = β̂/(1 + λ).&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;why-is-it-that-the-lasso-unlike-ridge-regression-results-in-coefficient-estimates-that-are-exactly-equal-to-zero&quot;&gt;Why is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?&lt;/h2&gt;
&lt;p&gt;Refer to page 221 of Introduction to Statistical Learning. Section- “&lt;em&gt;The Variable Selection Property of the Lasso&lt;/em&gt;”&lt;/p&gt;

&lt;h2 id=&quot;bayesian-interpretation-for-ridge-regression-and-the-lasso&quot;&gt;Bayesian Interpretation for Ridge Regression and the Lasso&lt;/h2&gt;
&lt;h2 id=&quot;what-is-confidence-interval-and-prediction-interval-in-linear-regression&quot;&gt;What is confidence interval and prediction interval in Linear Regression?&lt;/h2&gt;
&lt;h2 id=&quot;derive-equations-for-least-squares-in-vector--matrix-notation&quot;&gt;Derive equations for Least squares in vector &amp;amp; matrix notation&lt;/h2&gt;
&lt;h2 id=&quot;can-we-use-linear-regression-for-binary-classification&quot;&gt;Can we use Linear Regression for binary classification?&lt;/h2&gt;
&lt;p&gt;#&lt;/p&gt;
&lt;h1 id=&quot;classification&quot;&gt;Classification&lt;/h1&gt;
&lt;h2 id=&quot;some-approaches-for-classification&quot;&gt;Some approaches for classification&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Using linear regression of a Indicator Matrix&lt;/li&gt;
  &lt;li&gt;Linear Discriminant Analysis&lt;/li&gt;
  &lt;li&gt;Quadratic Discriminant Analysis&lt;/li&gt;
  &lt;li&gt;Regularized Discriminant Analysis &lt;strong&gt;&lt;em&gt;[Source: ESLR Page-112]&lt;/em&gt;&lt;/strong&gt; &lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.007.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-is-log-odds&quot;&gt;What is log odds?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.statisticshowto.com/log-odds/#:~:text=Taking%20the%20logarithm%20of%20the,p%2F(1%2Dp)%5D&quot;&gt;Log Oddds Definition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/https-towardsdatascience-com-what-and-why-of-log-odds-64ba988bf704&quot;&gt;What and Why of Log Odds&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The odds ratio is the probability of success/probability of failure. As an equation, that’s P(A)/P(-A), where P(A) is the probability of A, and P(-A) the probability of ‘not A’ (i.e. the complement of A).&lt;/p&gt;

&lt;p&gt;Taking the logarithm of the odds ratio gives us the log odds of A, which can be written as&lt;/p&gt;

&lt;p&gt;log(A) = log(P(A)/P(-A)),
Since the probability of an event happening, P(-A) is equal to the probability of an event not happening, 1 – P(A), we can write the log odds as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?log\frac{p}{(1-p)}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where:
p = the probability of an event happening
1 – p = the probability of an event not happening&lt;/p&gt;

&lt;h2 id=&quot;mle-estimation-for-logistic-regression&quot;&gt;MLE Estimation for Logistic Regression&lt;/h2&gt;
&lt;p&gt;Although we could use (non-linear) least squares to fit the logistic model , the more general method of maximum likelihood is preferred, since it has better statistical properties. In logistic regression, we use the logistic function,
&lt;img src=&quot;https://latex.codecogs.com/svg.image?p(X)=\frac{\exp^{\beta_0&amp;plus;\beta_1X}}{1&amp;plus;{\exp^{\beta_0&amp;plus;\beta_1X}}}&quot; /&gt;
&lt;img src=&quot;https://latex.codecogs.com/svg.image?\begin{aligned}p(X)&amp;amp;=\frac{\exp^{\beta_0&amp;plus;\beta_1X}}{1&amp;plus;{\exp^{\beta_0&amp;plus;\beta_1X}}}\cdots\cdots(1)&amp;space;\\1-p(X)&amp;amp;=1-\frac{\exp^{\beta_0&amp;plus;\beta_1X}}{1&amp;plus;{\exp^{\beta_0&amp;plus;\beta_1X}}}&amp;space;\\&amp;amp;=\frac{1&amp;plus;\exp^{\beta_0&amp;plus;\beta_1X}-\exp^{\beta_0&amp;plus;\beta_1X}}{1&amp;plus;{\exp^{\beta_0&amp;plus;\beta_1X}}}&amp;space;\\&amp;amp;=\frac{1}{1&amp;plus;{\exp^{\beta_0&amp;plus;\beta_1X}}}\cdots\cdots(2)&amp;space;\end{aligned}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dividing (1) by (2) &amp;amp; taking log on both sides,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?\begin{aligned}\frac{p(X)}{1-p(X)}&amp;amp;=\exp^{\beta_0&amp;plus;\beta_1X}\\log\frac{p(X)}{1-p(X)}&amp;amp;=\beta_0&amp;plus;\beta_1X\end{aligned}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let us make following parametric assumption:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?y_i|x_i&amp;space;=&amp;space;Bern(\sigma(w^Tx_i))\\where,&amp;space;\\\sigma(z)&amp;space;=&amp;space;\frac{1}{1&amp;plus;\exp{(-z)}}=\frac{\exp{(z)}}{1&amp;plus;\exp{(z)}}&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MLE is used to find the model parameters while maximizing,&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;***P(observed data&lt;/td&gt;
      &lt;td&gt;model parameters)***&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For Logistic Regression, we need to find the model parameter &lt;strong&gt;w&lt;/strong&gt; that maximizes conditional probability,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?=P(y_1,x_1,\cdots\cdots,y_n,x_n&amp;space;|&amp;space;\textbf{w})\\=\operatorname*{argmax}_w&amp;space;&amp;space;P(y_1,x_1,\cdots\cdots,y_n,x_n&amp;space;|&amp;space;\textbf{w})\\=\operatorname*{argmax}_w&amp;space;&amp;space;\prod_i^n&amp;space;P(y_i,x_i&amp;space;|&amp;space;\textbf{w})&amp;space;\cdots\cdots&amp;space;(Independence)\\=\operatorname*{argmax}_w&amp;space;&amp;space;\prod_i^n&amp;space;P(y_i&amp;space;|&amp;space;x_i,&amp;space;\textbf{w})P(x_i&amp;space;|&amp;space;\textbf{w})\\=\operatorname*{argmax}_w&amp;space;&amp;space;\prod_i^n&amp;space;P(y_i&amp;space;|&amp;space;x_i,&amp;space;\textbf{w})P(x_i)\cdots\cdots(x_i\,is\,independent\,of\,\textbf{w})\\=\operatorname*{argmax}_w&amp;space;&amp;space;\prod_i^n&amp;space;P(y_i&amp;space;|&amp;space;x_i,&amp;space;\textbf{w})\cdots\cdots(P(x_i)\,does\,not&amp;space;\,depend\,on\,\textbf{w})\\=\operatorname*{argmax}_w&amp;space;&amp;space;\prod_i^n&amp;space;\sigma{(w^Tx_i)}^{y_i}(1-{\sigma{(w^Tx_i)}})^{1-y_i}\\\\Equivalently,we\,would\,like\,to\,find\,the\,\textbf{w}\,to\,maximize\,the\,log\,likelihood:\\\\=\ln&amp;space;\prod_i^n{\sigma{(w^Tx_i)}^{y_i}(1-{\sigma{(w^Tx_i)}})^{1-y_i}}\\=\sum_{i}^{n}&amp;space;\ln({\sigma{(w^Tx_i)}^{y_i}(1-{\sigma{(w^Tx_i)}})^{1-y_i}})\\=\sum_{i}^{n}&amp;space;\ln{\sigma{(w^Tx_i)}^{y_i}&amp;plus;\ln(1-{\sigma{(w^Tx_i)}})^{1-y_i}}\\&amp;space;=\sum_{i}^{n}&amp;space;({y_i}\ln{\sigma{(w^Tx_i)}&amp;plus;({1-y_i})\ln(1-{\sigma{(w^Tx_i)}})})\\&amp;space;&amp;space;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources-to-understand-mle-estimation-for-logistic-regression&quot;&gt;Resources to understand MLE estimation for Logistic Regression&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zstevenwu.com/courses/s20/csci5525/resources/slides/lecture05.pdf&quot;&gt;lecture05.pdf (zstevenwu.com)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stat.rutgers.edu/home/pingli/papers/Logit.pdf&quot;&gt;Logit.dvi (rutgers.edu)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf&quot;&gt;ADAfaEPoV (cmu.edu)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/&quot;&gt;A Gentle Introduction to Logistic Regression With Maximum Likelihood Estimation (machinelearningmastery.com)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Logistic Regression and Maximum Likelihood Estimation Function&lt;/td&gt;
          &lt;td&gt;by Puja P. Pathak&lt;/td&gt;
          &lt;td&gt;CodeX&lt;/td&gt;
          &lt;td&gt;Medium](https://medium.com/codex/logistic-regression-and-maximum-likelihood-estimation-function-5d8d998245f9)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;difference-bw-logistic-regression--linear-discriminant-analysis&quot;&gt;Difference b/w Logistic Regression &amp;amp; Linear Discriminant Analysis&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-151]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Linear DA&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Parameters &lt;img src=&quot;https://latex.codecogs.com/svg.image?\beta_0,\beta_1&quot; /&gt; are estimated using Maximum Likelihood estimation&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Parameters are estimated using estimated mean &amp;amp; variance from normal distribution&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Decision boundary- Linear&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Decision boundary- Linear&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;logistic regression can outperform LDA if the Gaussian assumptions are not met&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some improvements over logistic regression when this assumption approximately holds.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;difference-bw-linear--quadratic-discriminant-analysis&quot;&gt;Difference b/w Linear &amp;amp; Quadratic Discriminant Analysis&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ESLR Page-109]&lt;/em&gt;&lt;/strong&gt; and &lt;a href=&quot;https://web.stanford.edu/class/stats202/content/lec9.pdf&quot;&gt;lecture9-stanford&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Linear DA&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Quadratic DA&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;All the classes have common covariance matrix Σ&lt;sub&gt;k&lt;/sub&gt; = Σ ∀ &lt;em&gt;k&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Each class has its own covariance matrix, Σ&lt;sub&gt;k&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Decision boundary- Linear&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Decision boundary- Quadratic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Discriminant Function&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.008.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&amp;lt;p&amp;gt;Discriminant Function&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.009.png&quot; alt=&quot;&quot; /&gt;&amp;lt;/p&amp;gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Since covariance matrices is common for all classes no such problem&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Since separate covariance matrices must be computed for each class, when p (#Features) is large, number of parameters increases dramatically.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-142]&lt;/em&gt;&lt;/strong&gt; LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance σ&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-142]&lt;/em&gt;&lt;/strong&gt; QDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and covariance matrix Σ&lt;sub&gt;k&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;With p predictors, estimating a covariance matrix requires estimating p(p+1)/2 parameters.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;With p predictors and K classses, estimating a covariance matrix requires estimating K.p(p+1)/2 parameters&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;LDA is a much less flexible classifier&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;QDA is a more flexible classifier&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Can have low variance high bias&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Can have high variance low bias&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;what-happens-when-the-classes-are-well-separated-in-logistic-regression&quot;&gt;What happens when the classes are well separated in Logistic Regression?&lt;/h2&gt;
&lt;p&gt;When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.
&lt;strong&gt;&lt;em&gt;[Source: ESLR, Page-128]&lt;/em&gt;&lt;/strong&gt; If the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates of the parameters are undefined (i.e., infinite; see Exercise 4.5). The LDA coefficients for the same data will be well defined, since the marginal likelihood will not permit these degeneracies.
&lt;a href=&quot;https://stats.stackexchange.com/questions/224863/understanding-complete-separation-for-logistic-regression&quot;&gt;https://stats.stackexchange.com/questions/224863/understanding-complete-separation-for-logistic-regression&lt;/a&gt;
&lt;a href=&quot;https://stats.stackexchange.com/questions/239928/is-there-any-intuitive-explanation-of-why-logistic-regression-will-not-work-for&quot;&gt;https://stats.stackexchange.com/questions/239928/is-there-any-intuitive-explanation-of-why-logistic-regression-will-not-work-for&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;compare-svm--logistic-regression&quot;&gt;Compare SVM &amp;amp; Logistic Regression&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-357]&lt;/em&gt;&lt;/strong&gt; SVM loss function is exactly zero for observations for which &lt;img src=&quot;https://latex.codecogs.com/svg.image?y_i(\beta_0&amp;plus;\beta_1x_{i1}&amp;plus;\cdots&amp;plus;\beta_px_{ip})&amp;gt;=1&amp;space;&quot; /&gt;  these correspond to observations that are on the correct side of the margin. In contrast, the loss function for logistic regression is not exactly zero anywhere. But it is very small for observations that are far from the decision boundary. Due to the similarities between their loss functions,  logistic regression and the support vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.&lt;/p&gt;

&lt;p&gt;#&lt;/p&gt;
&lt;h1 id=&quot;knn&quot;&gt;KNN&lt;/h1&gt;
&lt;h2 id=&quot;effect-of-k-on-training-error&quot;&gt;Effect of K on training error&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ESLR Page 15]&lt;/em&gt;&lt;/strong&gt; Error on the training data should be approximately an increasing function of k, and will always be 0 for k = 1. 
It appears that k-nearest-neighbor fits have a single parameter, the number of neighbors k, compared to the p parameters in least-squares fits. Although this is the case, we will see that the effective number of parameters of k-nearest neighbors is N/k and is generally bigger than p, and decreases with increasing k. To get an idea of why, note that if the neighborhoods were nonoverlapping, there would be N/k neighborhoods and we would fit one parameter (a mean) in each neighborhood. It is also clear that we cannot use sum-of-squared errors on the training&lt;/p&gt;

&lt;p&gt;set as a criterion for picking k, since we would always pick k = 1.&lt;/p&gt;
&lt;h2 id=&quot;how-does-bias--variance-vary-for-knn-with-the-choice-of-k&quot;&gt;How does bias &amp;amp; variance vary for KNN with the choice of K?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ESLR Page 40]&lt;/em&gt;&lt;/strong&gt; The choice of K has a drastic effect on the KNN classifier obtained. When K = 1, the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier. 
Just as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. With K = 1, the KNN training error rate is 0, but the test error rate may be quite high. In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not.&lt;/p&gt;
&lt;h1 id=&quot;svm&quot;&gt;SVM&lt;/h1&gt;

&lt;h2 id=&quot;what-is-hyperplane&quot;&gt;What is Hyperplane?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-68]&lt;/em&gt;&lt;/strong&gt;: In a p-dimensional space, a hyperplane is a flat affine subspace of hyperplane dimension p −1 For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In p &amp;gt; 3 dimensions, it can be hard to visualize a hyperplane, but the notion of a
(p − 1)-dimensional flat subspace still applies. The mathematical definition of a hyperplane is quite simple. In two dimensions, a hyperplane is defined by the equation&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.003.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In p-dimensional setting,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.004.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If a point &lt;img src=&quot;https://latex.codecogs.com/svg.image?X=(X_1,X_2,\cdots,X_p)^T&quot; /&gt; in p-dimensional space (i.e. a vector of length p) satisfies above eq., then X lies on the hyperplane.
Now, suppose that X does not satisfy the eq; rather,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.005.png&quot; alt=&quot;&quot; /&gt;, then, X lies to one side of the hyperplane&lt;/p&gt;

&lt;p&gt;On the other hand, If,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.95ba44c8-92c8-4d90-8a97-630964b6dcab.006.png&quot; alt=&quot;&quot; /&gt; then, X lies to the other side of the hyperplane.&lt;/p&gt;

&lt;p&gt;Some Resource on equation of line  &lt;a href=&quot;https://math.stackexchange.com/questions/2533114/equation-of-a-hyperplane-in-two-dimensions&quot;&gt;https://math.stackexchange.com/questions/2533114/equation-of-a-hyperplane-in-two-dimensions&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;variations-of-svm&quot;&gt;Variations of SVM&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Maximal Margin Classifier&lt;/li&gt;
  &lt;li&gt;Support Vector Classifier&lt;/li&gt;
  &lt;li&gt;Support Vector Machines&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;comparison-bw-maximal-margin-classifier-support-vector-classifier-support-vector-machine&quot;&gt;Comparison b/w Maximal Margin Classifier, Support Vector Classifier, Support Vector Machine&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;MM Classifier&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SVC&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SVM&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Used when separating hyperplane exist.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;generalization of the maximal margin classifier to the non-separable case&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generalization of SVC to non-separable &amp;amp; non-linear cases using Kernels&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;When separating hyperplane does not exist, the optimization problem has no solution with M &amp;gt; 0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Uses soft margin to identify hyperplane that almost separates the classes&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Uses soft margin to identify hyperplane that almost separates the classes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;An observation can only be on the right side of the margin, and the hyperplane&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;An observation can be not only on the wrong side of the margin, but also on the wrong side of the hyperplane.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;An observation can be not only on the wrong side of the margin, but also on the wrong side of the hyperplane.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Only observations that either lie on the margin or that violate the margin will affect the hyperplane, and hence the classifier obtained.&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;–Same as SVC–&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Changing the position of an observation that lies strictly on the correct side of the margin would not change the classifier at all, provided that its position remains on the correct side of the margin&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;–Same as SVC–&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;–Same as SVC–&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;When the tuning parameter C is large, then the margin is wide, many observations violate the margin, and so there are many support vectors. In this case, many observations are involved in determining the hyperplane. This classifier has low variance and potentially high bias. When C is small -» fewer support vector -» High variance, low bias&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;how-does-svm-select-support-vectors&quot;&gt;How does SVM select support vectors?&lt;/h2&gt;
&lt;h2 id=&quot;tree-based-models&quot;&gt;Tree Based Models&lt;/h2&gt;

&lt;h2 id=&quot;gini-index&quot;&gt;Gini Index&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.025.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{p}_{mk}&quot; /&gt; represents the proportion of training observations in the m&lt;sup&gt;th&lt;/sup&gt; region that are from the kth class. a measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the  &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{p}_{mk}&quot; /&gt;’s are close to zero or one. For this reason the Gini index is referred to as a measure of node purity—a small value indicates that a node contains predominantly observations from a single class.&lt;/p&gt;

&lt;h2 id=&quot;cross-entropy&quot;&gt;Cross-Entropy&lt;/h2&gt;

&lt;p&gt;Since 0 ≤ &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{p}_{mk}&quot; /&gt; ≤ 1, it follows that 0 ≤ −&lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{p}_{mk}&quot; /&gt; log &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{p}_{mk}&quot; /&gt;. One can show that the cross- entropy will take on a value near zero if the &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{p}_{mk}&quot; /&gt;’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is pure. In fact, it turns out that the Gini index and the cross-entropy are quite similar numerically.&lt;br /&gt;
&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.026.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-bagging-reduces-over-fitting-or-variance&quot;&gt;Why Bagging reduces over-fitting or variance?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-316]&lt;/em&gt;&lt;/strong&gt; Given a set of n independent observations Z&lt;sub&gt;1&lt;/sub&gt; , . . . , Z&lt;sub&gt;n&lt;/sub&gt;, each with variance σ&lt;sup&gt;2&lt;/sup&gt;, the variance of the mean &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{Z}&quot; /&gt; of the observations is given by σ&lt;sup&gt;2&lt;/sup&gt;/n. In other words, averaging a set of observations reduces variance.&lt;/p&gt;

&lt;p&gt;Explanation of why the above happens: &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance#Properties&quot;&gt;https://en.wikipedia.org/wiki/Variance#Properties&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hence a natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we could calculate &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{f}^1(x),\hat{f}^2(x),\cdots,\hat{f}^B(x)&quot; /&gt; using &lt;strong&gt;B&lt;/strong&gt; separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.027.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate B different bootstrapped training data sets. We then train our method on the bth bootstrapped training set in order to get &lt;img src=&quot;https://latex.codecogs.com/svg.image?\hat{f}^{*B}(x)&quot; /&gt;, and finally average all the predictions, to obtain&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.028.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;oob-error-estimation&quot;&gt;OOB Error Estimation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-317]&lt;/em&gt;&lt;/strong&gt; The key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we can average these predicted responses (if regression is the goal) or can take a  majority vote (if classification is the goal). This leads to a single OOB prediction for the ith observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification  problem) can be computed. The resulting OOB error is a valid estimate of the test error for the  bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.&lt;/p&gt;

&lt;h2 id=&quot;how-random-forests-ensure-that-trees-are-decorrelated&quot;&gt;How Random Forests ensure that trees are decorrelated?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-320]&lt;/em&gt;&lt;/strong&gt; Random forests provide an improvement over bagged trees by way of a random small tweak  that decorrelates the trees. As in bagging, we build a number  forest of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of √m predictors is taken at each split, and typically we choose m ≈ p—that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data). In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting. Random forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average (p − m)/p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable. The main difference between bagging and random forests is the choice of predictor subset size m. For instance, if a random forest is built using m = p, then this amounts simply to bagging.&lt;/p&gt;

&lt;h2 id=&quot;does-random-forest-overfit-if-we-increase-the-number-of-trees&quot;&gt;Does Random Forest overfit if we increase the number of trees?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-321]&lt;/em&gt;&lt;/strong&gt; As with bagging, random forests will not overfit if we increase B, so in practice we use a value of B sufficiently large for the error rate to have settled down.&lt;/p&gt;

&lt;h2 id=&quot;boosting&quot;&gt;Boosting&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kho6oANGu_A&quot;&gt;Boosting Machine Learning Tutorial | Adaptive Boosting, Gradient Boosting, XGBoost | Edureka - YouTube&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;does-boosting-overfit-if-we-increase-the-number-of-trees&quot;&gt;Does Boosting overfit if we increase the number of trees?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-323]&lt;/em&gt;&lt;/strong&gt; Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B. (B is the number of trees)&lt;/p&gt;

&lt;h2 id=&quot;compare-dt-bagging-rf-boosting&quot;&gt;Compare DT, Bagging, RF, Boosting&lt;/h2&gt;

&lt;p&gt;|Model|Description|
| :– | :– |
|Bagging|We build a number of decision trees on bootstrapped training samples using all the predictors|
|RF|&amp;lt;p&amp;gt;We build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split, and typically we choose m ≈ √p. &amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Why does RF reduce more variance as compared to Bagging? Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in  variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance  over a single tree in this setting. Random forests overcome this  problem by forcing each split to consider only a subset of the predictors. Therefore, on average (p − m)/p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.&amp;lt;/p&amp;gt;|
|Boosting|Boosting works similar to Bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.|&lt;/p&gt;
&lt;h2 id=&quot;cross-validation&quot;&gt;Cross Validation&lt;/h2&gt;

&lt;h2 id=&quot;compare-loocvleave-one-out-cross-validation-with-k-fold-cv&quot;&gt;Compare LOOCV(Leave One Out Cross Validation) with K-Fold CV&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-180]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;|LOOCV|K-Fold CV|
| :– | :– |
|LOOCV has the potential to be expensive to implement, since the model has to be fit n times. This can be very time consuming if n is large, and if each individual model is slow to fit.|Computationally less expensive|
|each training set contains n − 1 observations - has lower bias|each training set contains (k − 1)n/k observations—fewer than in the LOOCV approach - has higher bias|
|LOOCV has higher variance than does k-fold CV with k&amp;lt;n. When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other|&amp;lt;p&amp;gt;When we perform k-fold CV with k&amp;lt;n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.&amp;lt;/p&amp;gt;|&lt;/p&gt;
&lt;h2 id=&quot;classification-evaluation-metrics&quot;&gt;Classification Evaluation Metrics&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Sensitivity = Recall = True Positive Rate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?\frac{TP}{TP&amp;plus;FN}&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Specificity = True Negative Rate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?\frac{TN}{TN&amp;plus;FP}&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Precision&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?\frac{TP}{TP&amp;plus;FP}&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;False Positive Rate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/svg.image?\frac{FP}{FP&amp;plus;TN}&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lift&quot;&gt;Lift&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lift_\(data_mining\)&quot;&gt;Lift (data mining) - Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://howtolearnmachinelearning.com/articles/the-lift-curve-in-machine-learning/&quot;&gt;The Lift Curve in Machine Learning explained | Learn Machine Learning (howtolearnmachinelearning.com)&lt;/a&gt;
    &lt;h2 id=&quot;pca&quot;&gt;PCA&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;does-scaling-affect-pca-outcome&quot;&gt;Does scaling affect PCA outcome?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Source: ISLR Page-381]&lt;/em&gt;&lt;/strong&gt; If in a dataset variables are measured on different scales, then the variable with large values might have high variance. If we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for this high variance variable, since that variable has by far the highest variance.&lt;/p&gt;

&lt;p&gt;It is undesirable for the principal components obtained to depend on an arbitrary choice of scaling, and hence, we typically scale each variable to have standard deviation one before we perform PCA.&lt;/p&gt;

&lt;h2 id=&quot;what-are-loading-vectors-and-score-vectors&quot;&gt;What are loading vectors and score vectors?&lt;/h2&gt;
&lt;h2 id=&quot;deep-learning&quot;&gt;Deep Learning&lt;/h2&gt;

&lt;h2 id=&quot;how-does-dropout-work-in-deep-learning-algorithms&quot;&gt;How does dropout work in deep learning algorithms?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Dropout Regularization - Practical Aspects of Deep Learning&lt;/td&gt;
          &lt;td&gt;Coursera](https://www.coursera.org/lecture/deep-neural-network/dropout-regularization-eM33A)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Why Dropout is so effective in Deep Neural Network?&lt;/td&gt;
          &lt;td&gt;Towards Data Science](https://towardsdatascience.com/introduction-to-dropout-to-regularize-deep-neural-network-8e9d6b1d4386)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-is-the-vanishing--exploding-gradient-problem-how-to-identify-it-how-is-it-solved&quot;&gt;What is the vanishing &amp;amp; exploding gradient problem? How to identify it? How is it solved?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[The Vanishing/Exploding Gradient Problem in Deep Neural Networks&lt;/td&gt;
          &lt;td&gt;by Kurtis Pykes&lt;/td&gt;
          &lt;td&gt;Towards Data Science](https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[The Vanishing Gradient Problem. The Problem, Its Causes, Its…&lt;/td&gt;
          &lt;td&gt;by Chi-Feng Wang&lt;/td&gt;
          &lt;td&gt;Towards Data Science](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of exploding gradient. Alternatively, if the derivatives are small then the gradient will decrease exponentially as we propagate through the model until it eventually vanishes, and this is the vanishing gradient problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Identifying Exploding gradient:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model is not learning much on the training data therefore resulting in a poor loss.&lt;/li&gt;
  &lt;li&gt;The model will have large changes in loss on each update due to the models instability.&lt;/li&gt;
  &lt;li&gt;The models loss will be NaN during training.&lt;/li&gt;
  &lt;li&gt;Model weights grow exponentially and become very large when training the model.&lt;/li&gt;
  &lt;li&gt;The model weights become NaN in the training phase.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Identifying Vanishing gradient:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model will improve very slowly during the training phase and it is also possible that training stops very early, meaning that any further training does not improve the model.&lt;/li&gt;
  &lt;li&gt;The weights closer to the output layer of the model would witness more of a change whereas the layers that occur closer to the input layer would not change much (if at all).&lt;/li&gt;
  &lt;li&gt;Model weights shrink exponentially and become very small when training the model.&lt;/li&gt;
  &lt;li&gt;The model weights become 0 in the training phase.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Solving gradient problem:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Reducing the amount of Layers&lt;/li&gt;
  &lt;li&gt;Choosing a small learning rate so that there are no large updates in the gradients&lt;/li&gt;
  &lt;li&gt;Gradient Clipping (Exploding Gradients)&lt;/li&gt;
  &lt;li&gt;Weight Initialization&lt;/li&gt;
  &lt;li&gt;use other activation functions, such as ReLU, which doesn’t cause a small derivative.&lt;/li&gt;
  &lt;li&gt;Residual networks&lt;/li&gt;
  &lt;li&gt;Add batch normalization layers&lt;/li&gt;
  &lt;li&gt;Use LSTM Networks&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-is-gradient-clipping&quot;&gt;What is Gradient Clipping?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Introduction to Gradient Clipping Techniques with Tensorflow&lt;/td&gt;
          &lt;td&gt;cnvrg.io](https://cnvrg.io/gradient-clipping/#:~:text=%20Gradient%20clipping%20can%20be%20applied%20in%20two,by%20value%202%20Clipping%20by%20norm%20More%20)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem&quot;&gt;Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem) - neptune.ai&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Gradient clipping involves forcing the gradients to a certain number when they go above or below a defined threshold.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Types of Clipping techniques&lt;/strong&gt;
Gradient clipping can be applied in two common ways:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clipping by value&lt;/strong&gt; : If a gradient exceeds some threshold value, we clip that gradient to the threshold. If the gradient is less than the lower limit then we clip that too, to the lower limit of the threshold. The algorithm is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;g ← ∂C/∂W
if ‖g‖ ≥ max_threshold or ‖g‖ ≤ min_threshold then
    g ← threshold (accordingly)
end if where max\_threshold and min\_threshold are the boundary values and between them lies a range of values that gradients can take. g, here is the gradient, and  ‖g ‖ is the norm of g. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Clipping by norm&lt;/strong&gt;: We clip the gradients by multiplying the unit vector of the gradients with the threshold. 
The algorithm is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;g ← ∂C/∂W
if ‖g‖ ≥ threshold then
    g ← threshold * g/‖g‖
end if where the threshold is a hyperparameter, g is the gradient, and  ‖g ‖ is the norm of g. Since g/‖ g‖ is a unit vector, after rescaling the new g will have norm equal to threshold. Note that if  ‖g‖ &amp;lt; c, then we don’t need to do anything,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Gradient clipping ensures the gradient vector g has norm at most equal to threshold.&lt;/p&gt;

&lt;h2 id=&quot;compare-activation-functions&quot;&gt;Compare Activation Functions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Activation Functions: Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax&lt;/td&gt;
          &lt;td&gt;by Mukesh Chaudhary&lt;/td&gt;
          &lt;td&gt;Medium](https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;compare-loss-functions&quot;&gt;Compare Loss Functions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Loss Function&lt;/td&gt;
          &lt;td&gt;Loss Function In Machine Learning (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/)&lt;img src=&quot;Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.030.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;compare-optimizers&quot;&gt;Compare Optimizers&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent&quot;&gt;Line to coursera&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Gradient Descent&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent - Same as Gradient descent, but for each sample&lt;/li&gt;
  &lt;li&gt;Mini Batch Gradient Descent - Same as Gradient descent, but for each mini batch&lt;/li&gt;
  &lt;li&gt;Gradient Descent with Momentum - Each update is not made with the current value of gradient, but with the exponential moving average of the gradients:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.031.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.032.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Root Mean Square Propagation (RMSProp):&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.033.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Adaptive Moment Estimate (Adam)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/ds-interview/Aspose.Words.b499c499-1cb7-4a67-bd0b-7d4698e5c020.034.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-reduce-overfitting&quot;&gt;How to reduce overfitting&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Adding more data&lt;/li&gt;
  &lt;li&gt;Data Augmentation&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;Early Stopping&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;cnn&quot;&gt;CNN&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ArPaAX_PhIs&amp;amp;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&amp;amp;index=1&quot;&gt;(302) C4W1L01 Computer Vision - YouTube&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2021/07/word2vec-for-word-embeddings-a-beginners-guide/&quot;&gt;Word2Vec For Word Embeddings -A Beginner’s Guide - Analytics Vidhya&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;glovec&quot;&gt;GloVec&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;glove.pdf (stanford.edu)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;positional-embeddings&quot;&gt;Positional Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=eEGDEJfP74k&quot;&gt;https://www.youtube.com/watch?v=eEGDEJfP74k&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py&quot;&gt;https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9 429b5c40a139/tensor2tensor/layers/common_attention.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb&quot;&gt;https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transforme r/transformer_positional_encoding_graph.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;transformers&quot;&gt;Transformers&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=QUk6jvB9RGk&quot;&gt;https://www.youtube.com/watch?v=QUk6jvB9RGk&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bertbidirectional-encoder-representations-from-transformers&quot;&gt;BERT(Bidirectional Encoder Representations from Transformers)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://jalammar.github.io/illustrated-bert/&quot;&gt;The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/&quot;&gt;A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-q-network&quot;&gt;Deep Q Network&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;Human-level control through deep reinforcement learning (storage.googleapis.com) &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/rl/ipynb/deep_q_network_breakout.ipynb#scrollTo=2gc-Xf33zqNW&quot;&gt;deep_q_network_breakout - Colaboratory (google.com)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://keras.io/examples/rl/deep_q_network_breakout/#train&quot;&gt;Deep Q-Learning for Atari Breakout (keras.io)&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[My Journey Into Deep Q-Learning with Keras and Gym&lt;/td&gt;
      &lt;td&gt;by Gaetan Juvin&lt;/td&gt;
      &lt;td&gt;Medium](https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DhdUlDIAG7Y&quot;&gt;Q Learning Explained | Reinforcement Learning Using Python | Q Learning in AI | Edureka - YouTube&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;what-are-the-different-types-of-sampling-techniques&quot;&gt;What are the different types of sampling techniques?&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Random Sampling&lt;/li&gt;
  &lt;li&gt;Systematic Sampling&lt;/li&gt;
  &lt;li&gt;Stratified Sampling vs Cluster Sampling
    &lt;ol&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;[Difference Between Stratified Sampling and Cluster Sampling&lt;/td&gt;
              &lt;td&gt;Compare the Difference Between Similar Terms](https://www.differencebetween.com/difference-between-stratified-and-vs-cluster-sampling/#:~:text=%20Difference%20Between%20Stratified%20Sampling%20and%20Cluster%20Sampling,include%20homogenous%20members%20while%2C%20in%20cluster…%20More%20)&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;[Cluster Sampling: Definition, Method and Examples&lt;/td&gt;
              &lt;td&gt;QuestionPro](https://www.questionpro.com/blog/cluster-sampling/)&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Judgemental or Purposive Sampling
    &lt;ol&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.formpl.us/blog/purposive-sampling&quot;&gt;Purposive Sampling: Definition, Types, Examples (formpl.us)&lt;/a&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;systematic-sampling&quot;&gt;Systematic Sampling&lt;/h2&gt;
&lt;p&gt;Systematic sampling is a type of probability &lt;a href=&quot;https://www.investopedia.com/terms/s/sampling-distribution.asp&quot;&gt;sampling method&lt;/a&gt; in which sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval. This interval, called the sampling interval, is calculated by dividing the population size by the desired sample size. Despite the sample population being selected in advance, systematic sampling is still thought of as being random if the periodic interval is determined beforehand and the starting point is random.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KEY TAKEAWAYS&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Systematic sampling is a probability sampling method in which a random sample, with a fixed periodic interval, is selected from a larger population.&lt;/li&gt;
  &lt;li&gt;The fixed periodic interval, called the sampling interval, is calculated by dividing the population size by the desired sample size.&lt;/li&gt;
  &lt;li&gt;Other advantages of this methodology include eliminating the phenomenon of clustered selection and a low probability of contaminating data.&lt;/li&gt;
  &lt;li&gt;Disadvantages include over- or under-representation of particular patterns and a greater risk of data manipulation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;entropy--information-gain&quot;&gt;Entropy &amp;amp; Information Gain&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.humaneer.org/blog/data-science-information-gain-and-entropy-explained/&quot;&gt;Information Gain and Entropy Explained | Data Science - Humaneer&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;hypothesis-testing&quot;&gt;Hypothesis Testing&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.w3schools.com/statistics/statistics_hypothesis_testing.php&quot;&gt;Statistics - Hypothesis Testing (w3schools.com)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A statistical hypothesis is an assumption about a population which may or may not be true. Hypothesis testing is a set of formal procedures used by statisticians to either accept or reject statistical hypotheses. Statistical hypotheses are of two types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Null hypothesis, H&lt;sub&gt;0&lt;/sub&gt; - represents a hypothesis of chance basis.&lt;/li&gt;
  &lt;li&gt;Alternative hypothesis, H&lt;sub&gt;a&lt;/sub&gt; - represents a hypothesis of observations which are influenced by some non-random cause.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hypothesis testing is an act in statistics whereby an analyst &lt;a href=&quot;https://www.investopedia.com/terms/w/wilcoxon-test.asp&quot;&gt;tests&lt;/a&gt; an assumption regarding a population parameter. The methodology employed by the analyst depends on the nature of the data used and the reason for the analysis.&lt;/p&gt;

&lt;p&gt;Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data. Such data may come from a larger population, or from a data-generating process. The word “population” will be used for both of these cases in the following descriptions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KEY TAKEAWAYS&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data.&lt;/li&gt;
  &lt;li&gt;The test provides evidence concerning the plausibility of the hypothesis, given the data.&lt;/li&gt;
  &lt;li&gt;Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-p-value&quot;&gt;What is p-value?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.scribbr.com/statistics/p-value/&quot;&gt;Understanding P-values | Definition and Examples (scribbr.com) &lt;/a&gt;&lt;a href=&quot;https://www.investopedia.com/terms/p/p-value.asp&quot;&gt;P-Value Definition (investopedia.com)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;type-i--type-ii-error&quot;&gt;Type I &amp;amp; Type II Error&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Type I error&lt;/strong&gt;, also known as a &lt;strong&gt;“false positive”:&lt;/strong&gt; the error of rejecting a null hypothesis when it is actually true. In other words, this is the error of accepting an alternative hypothesis (the real hypothesis of interest) when the results can be attributed to chance. Plainly speaking, it occurs when we are observing a difference when in truth there is none (or more specifically - no statistically significant difference). So the probability of making a type I error in a test with rejection region R is 0 P(R&lt;/td&gt;
      &lt;td&gt;H&lt;sub&gt;0&lt;/sub&gt; is true) .&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Type II error&lt;/strong&gt;, also known as a &lt;strong&gt;“false negative”:&lt;/strong&gt; the error of not rejecting a null hypothesis when the alternative hypothesis is the true state of nature. In other words, this is the error of failing to accept an alternative hypothesis when you don’t have adequate power. Plainly speaking, it occurs when we are failing to observe a difference when in truth there is one. So the probability of making a type II error in a test with rejection region R is 1 - P (R&lt;/td&gt;
      &lt;td&gt;H&lt;sub&gt;a&lt;/sub&gt; is true). The power of the test can be P (R&lt;/td&gt;
      &lt;td&gt;H&lt;sub&gt;a&lt;/sub&gt; is true).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;normal-distribution&quot;&gt;Normal distribution:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.w3schools.com/statistics/statistics_standard_normal_distribution.php&quot;&gt;Statistics - Standard Normal Distribution (w3schools.com)&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[Normal Distribution&lt;/td&gt;
      &lt;td&gt;Examples, Formulas, &amp;amp; Uses (scribbr.com)](https://www.scribbr.com/statistics/normal-distribution/)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2021/05/normal-distribution-an-ultimate-guide/&quot;&gt;Normal Distribution | What is Normal Distribution : An Ultimate Guide (analyticsvidhya.com)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;h2 id=&quot;explain-boxplots&quot;&gt;Explain Boxplots&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51&quot;&gt;Understanding Boxplots. The image above is a boxplot. A boxplot… | by Michael Galarnyk | Towards Data Science&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[Explaining the 68-95-99.7 rule for a Normal Distribution&lt;/td&gt;
      &lt;td&gt;by Michael Galarnyk&lt;/td&gt;
      &lt;td&gt;Towards Data Science](https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;explain-qq-plots&quot;&gt;Explain QQ plots&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2021/09/q-q-plot-ensure-your-ml-model-is-based-on-the-right-distributions/&quot;&gt;Q-Q plot - Ensure Your ML Model is Based on the Right Distribution (analyticsvidhya.com)&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[Q-Q Plots Explained. Explore the powers of Q-Q plots.&lt;/td&gt;
      &lt;td&gt;by Paras Varshney&lt;/td&gt;
      &lt;td&gt;Towards Data Science](https://towardsdatascience.com/q-q-plots-explained-5aa8495426c0)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot&quot;&gt;Q–Q plot - Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;techniques-for-missing-value-imputation&quot;&gt;Techniques for Missing value imputation&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Descriptive statistics like mean, median, mode, or constant value&lt;/li&gt;
  &lt;li&gt;Using regression/classification to predict the missing values (sklearn IterativeImputer)&lt;/li&gt;
  &lt;li&gt;Regression techniques are used to interpolate/extrapolate missing values. &lt;a href=&quot;https://www.statology.org/interpolation-vs-extrapolation/&quot;&gt;Interpolation vs. Extrapolation: What’s the Difference? - Statology&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;feature-scaling-and-normalization&quot;&gt;Feature Scaling and Normalization&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://sebastianraschka.com/Articles/2014_about_feature_scaling.html&quot;&gt;About Feature Scaling and Normalization (sebastianraschka.com)&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Z-score = x-mean(x)/std(x) : mean = 0, std = 1 for the new data. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.&lt;/li&gt;
  &lt;li&gt;Min max scaling = (X - Xmin)/(Xmax - Xmin) : range of new data = 0,1&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Reduce the skewness of data using log,exponential, square root&lt;/li&gt;
  &lt;li&gt;Skewness refers to the presence of outliers in the data stretching towards right or left of the normal distribution&lt;/li&gt;
  &lt;li&gt;Kurtosis is the measure of sharpness of the peak of the normal distribution&lt;/li&gt;
  &lt;li&gt;Use &lt;strong&gt;spatial sign&lt;/strong&gt; for multiple predictors:&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;handling-outliers&quot;&gt;Handling Outliers&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Detection:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using z-score&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using box plots&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Q1 = np.percentile(data, 25, interpolation = &apos;midpoint&apos;)
 Q2 = np.percentile(data, 50, interpolation = &apos;midpoint&apos;)
 Q3 = np.percentile(data, 75, interpolation = &apos;midpoint&apos;)
 IQR = Q3-Q1
 Outlier = Q1 – 1.5IQR and Q3 + 1.5QIQR
 print(&apos;Interquartile range is&apos;, IQR)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Treatment&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Drop the outlier records&lt;/li&gt;
  &lt;li&gt;Cap your outlier data&lt;/li&gt;
  &lt;li&gt;Assign a new value&lt;/li&gt;
  &lt;li&gt;Try a new transformation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;encoding-categorical-variables&quot;&gt;Encoding Categorical Variables&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;One hot encoding&lt;/strong&gt;:  how to do it for large vectors? [How to Handle Categorical Features&lt;/td&gt;
          &lt;td&gt;by Ashutosh Sahu&lt;/td&gt;
          &lt;td&gt;Analytics Vidhya&lt;/td&gt;
          &lt;td&gt;Medium](https://medium.com/analytics-vidhya/how-to-handle-categorical-features-ab65c3cf498e)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;One hot encoding with Multiple Categories&lt;/strong&gt;: In this technique, instead of creating the new column for every category, they limit creating the new column for 10 most frequent categories.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ordinal Number Encoding:&lt;/strong&gt; In this technique, each unique category value is given an integer value. For instance, “red” equals 1, “green” equals 2 and “blue” equals 3.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Count or Frequency Encoding:&lt;/strong&gt; In this technique we will substitute the categories by the count of the observations that show that category in the dataset&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Target Guided Ordinal Encoding:&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;Choose a categorical variable.&lt;/li&gt;
      &lt;li&gt;Take the aggregated mean of the categorical variable and apply it to the target variable.&lt;/li&gt;
      &lt;li&gt;Assign higher integer values or a higher rank to the category with the highest mean.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mean Ordinal Encoding:&lt;/strong&gt; Replace the category with the obtained mean value instead of assigning integer values to it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Probability Ratio Encoding:&lt;/strong&gt; This technique is suitable for classification problems only when the target variable is binary(Either 1 or 0 or True or False). In this technique, we will substitute the category value with the probability ratio i.e. P(1)/P(0).
    &lt;ol&gt;
      &lt;li&gt;Using the categorical variable, evaluate the probability of the Target variable (where the output is True or 1).&lt;/li&gt;
      &lt;li&gt;Calculate the probability of the Target variable having a False or 0 output.&lt;/li&gt;
      &lt;li&gt;Calculate the probability ratio i.e. P(True or 1) / P(False or 0).&lt;/li&gt;
      &lt;li&gt;Replace the category with a probability ratio.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Weight of Evidence&lt;/strong&gt; Explained later&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Label Encoding&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;weight-of-evidence--information-value&quot;&gt;Weight of Evidence &amp;amp; Information Value&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html#:~:text=The%20WOE%20should%20be%20monotonic%2C%20i.e.%20either%20growing,smoothing%20-%20the%20fewer%20bins%2C%20the%20more%20smoothing.&quot;&gt;Weight of Evidence (WOE) and Information Value (IV) Explained (listendata.com)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;multivariate-analysis&quot;&gt;Multivariate analysis&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Add additional variables to the chart using hue&lt;/li&gt;
  &lt;li&gt;Add additional variables to the chart using columns&lt;/li&gt;
  &lt;li&gt;Using FacetGrid&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;some-basic-eda-steps&quot;&gt;Some basic EDA steps&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Identifying bad and anamoulous data&lt;/li&gt;
  &lt;li&gt;Identifying not usefule data - duplicate data, 0 variance features, identifier fields&lt;/li&gt;
  &lt;li&gt;Identifying features with only 25-30% data&lt;/li&gt;
  &lt;li&gt;Look at the data types if they are correct&lt;/li&gt;
  &lt;li&gt;Check for duplicates&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;common-analytics-functions&quot;&gt;Common Analytics Functions&lt;/h2&gt;

&lt;p&gt;First_value, last_value, nth_value, lead, lag, rank, dense_rank, cume_dist, percent_value&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sparkbyexamples.com/spark/spark-sql-window-functions/&quot;&gt;Spark Window Functions with Examples - Spark by {Examples} (sparkbyexamples.com)&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[Top 5 SQL Analytic Functions Every Data Analyst Needs to Know&lt;/td&gt;
      &lt;td&gt;by Dario Radečić&lt;/td&gt;
      &lt;td&gt;Towards Data Science](https://towardsdatascience.com/top-5-sql-analytic-functions-every-data-analyst-needs-to-know-3f32788e4ebb)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[SQL Functions&lt;/td&gt;
      &lt;td&gt;SQL Functions For Data Analysis (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2020/07/sql-functions-for-data-analysis-tasks/)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions&quot;&gt;Built-in Functions - Spark 3.2.1 Documentation (apache.org)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;explaing-cut-and-quantile-function-in-python&quot;&gt;Explaing Cut and quantile function in python&lt;/h2&gt;
&lt;h2 id=&quot;which-algorithms-are-sensitive-to-feature-scaling-and-normalizationoutlierscategorical-variablemissing-values&quot;&gt;Which algorithms are sensitive to feature scaling and normalization/outliers/categorical variable/missing values&lt;/h2&gt;
&lt;h2 id=&quot;what-is-variable-clustering&quot;&gt;What is Variable Clustering?&lt;/h2&gt;
&lt;h2 id=&quot;how-to-analyse-datasets-with-very-large-number-of-features&quot;&gt;How to analyse datasets with very large number of features?&lt;/h2&gt;
&lt;h2 id=&quot;how-to-plot-two-categorical-variables&quot;&gt;How to plot two categorical variables&lt;/h2&gt;
&lt;h2 id=&quot;how-to-scatter-plot-bw-all-variables&quot;&gt;How to scatter plot b/w all variables&lt;/h2&gt;
&lt;h2 id=&quot;what-is-heatmap&quot;&gt;What is heatmap?&lt;/h2&gt;
&lt;h2 id=&quot;how-to-create-correlation-plot-in-python&quot;&gt;How to create correlation plot in python?&lt;/h2&gt;
&lt;h2 id=&quot;how-to-do-missing-value-imputation-for-time-series-data&quot;&gt;How to do missing value imputation for time series data?&lt;/h2&gt;

&lt;h2 id=&quot;time-series-forecasting&quot;&gt;Time Series Forecasting&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/#:~:text=11%20Classical%20Time%20Series%20Forecasting%20Methods%20in%20Python,process%20at%20prior%20time%20...%20More%20items...%20&quot;&gt;11 Classical Time Series Forecasting Methods in Python (Cheat Sheet) (machinelearningmastery.com)&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;case-study&quot;&gt;Case Study&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://bytes.swiggy.com/item2vec-with-metadata-incorporating-side-information-in-item-embeddings-167fb8d3f404&quot;&gt;Item2Vec with Metadata: incorporating side-information in item embeddings | by Alapatimanoharsai | Swiggy Bytes&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h1&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[Dynamic Programming&lt;/td&gt;
      &lt;td&gt;Introduction - YouTube](https://www.youtube.com/watch?v=nqowUJzG-iM&amp;amp;list=PL_z_8CaSLPWekqhdCPmFohncHwz8TY2Go&amp;amp;index=1)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oBt53YbR9Kk&quot;&gt;Dynamic Programming - Learn to Solve Algorithmic Problems &amp;amp; Coding Challenges - YouTube&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 27 May 2022 23:19:34 +0530</pubDate>
        <link>http://localhost:4000/blog/2022/05/27/ds-interview-prep</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2022/05/27/ds-interview-prep</guid>
        
        
        <category>data</category>
        
        <category>science,</category>
        
        <category>machine</category>
        
        <category>learning,</category>
        
        <category>interview</category>
        
        <category>DS Interview</category>
        
      </item>
    
      <item>
        <title>Tensorflow-Extract vocabulary from tensorflow VocabularyProcessor object</title>
        <description>&lt;h4 id=&quot;code&quot;&gt;Code&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This is a cat&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;This must be boy&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;This is a a dog&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_document_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## Create the vocabularyprocessor object, setting the max lengh of the documents.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_processor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VocabularyProcessor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_document_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## Transform the documents using the vocabulary.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;    

&lt;span class=&quot;c1&quot;&gt;## Extract word:id mapping from the object.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_processor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocabulary_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_mapping&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## Sort the vocabulary dictionary on the basis of values(id).
## Both statements perform same task.
#sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted_vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## Treat the id&apos;s as index into list and create a list of words in the ascending order of id&apos;s
## word with id i goes at index i of the list.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocabulary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Vocabulary : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocabulary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Transformed documents : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sample output :&lt;/p&gt;

&lt;p&gt;Vocabulary : \&lt;/p&gt;

&lt;p&gt;[‘&lt;UNK&gt;’, ‘This’, ‘is’, ‘a’, ‘cat’, ‘must’, ‘be’, ‘boy’, ‘dog’] \&lt;/UNK&gt;&lt;/p&gt;

&lt;p&gt;Transformed documents : \&lt;/p&gt;

&lt;p&gt;[[1 2 3 4 0] &lt;br /&gt;
[1 5 6 7 0] &lt;br /&gt;
[1 2 3 3 8]]&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page&apos;s canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page&apos;s unique identifier variable
};
*/
(function() { // DON&apos;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&apos;script&apos;);
s.src = &apos;//agarnitin86-github-io.disqus.com/embed.js&apos;;
s.setAttribute(&apos;data-timestamp&apos;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
</description>
        <pubDate>Sat, 25 Dec 2021 15:19:34 +0530</pubDate>
        <link>http://localhost:4000/blog/2021/12/25/tensorflow-vocab</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/12/25/tensorflow-vocab</guid>
        
        
        <category>python,</category>
        
        <category>tensorflow,</category>
        
        <category>nlp,</category>
        
        <category>gpu</category>
        
        <category>Tensorflow</category>
        
      </item>
    
      <item>
        <title>Anaphora resolution for binary entities using BERT - Part-1</title>
        <description>&lt;p&gt;&lt;strong&gt;&lt;em&gt;Anaphora resolution&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;can be defined as the task of resolving pronouns like&lt;/em&gt; &lt;strong&gt;&lt;em&gt;he, she, you, me, I, we, us, this, them, that, etc&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;to their matching entity in given context.&lt;/em&gt; Context here can be same sentence, paragraph or a larger piece of text. Entity can be a named entity like name of person, place, organization, etc, or in more complex settings it can be noun phrase, verb phrase, sentence, or even whole paragraph. It may not be the final task of NLP problems, but, it is an important task for many high level NLP tasks such as document summarization, question answering, information extraction, aspect based sentiment analysis, etc.&lt;/p&gt;

&lt;p&gt;For example: “I have been using &lt;strong&gt;«brand_name»&lt;/strong&gt; since last 4 years. &lt;strong&gt;Their&lt;/strong&gt; products are really good.”
To attribute the sentiment of second sentence to «brand_name» we will need to resolve &lt;strong&gt;their&lt;/strong&gt;, which refers to «brand_name»&lt;/p&gt;

&lt;p&gt;Primary types&lt;sup&gt;[1]&lt;/sup&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pronominal: This is the most common type where a referent is referred by a pronoun.
Example: “John found the love of his life” where ‘his’ refers to ‘John’.&lt;/li&gt;
  &lt;li&gt;Definite noun phrase: The antecedent is referred by a phrase of the form “&amp;lt;the&amp;gt; &amp;lt;noun phrase&amp;gt;”.
Continued example: “The relationship did not last long”, where ‘The relationship’ refers
to ‘the love’ in the preceding sentence.&lt;/li&gt;
  &lt;li&gt;Quantifier/Ordinal: The anphor is a quantifier such as ‘one’ or an ordinal such as ‘first’.
Continued Example: “He started a new one” where ‘one’ refers to ‘The relationship’
(effectively meaning ‘a relationship’).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cataphora&lt;/strong&gt; : when anaphora &lt;em&gt;precedes&lt;/em&gt; the antecedent&lt;/p&gt;

&lt;p&gt;For eg.,
Because &lt;strong&gt;&lt;em&gt;she&lt;/em&gt;&lt;/strong&gt; was going to the departmental store, &lt;strong&gt;&lt;em&gt;Mary&lt;/em&gt;&lt;/strong&gt; was asked to pick up the vegetables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BERT (Bidirectional Encoder Representations from Transformers)&lt;sup&gt;[2]&lt;/sup&gt;&lt;/strong&gt; is one of latest developments in the field of NLP by Google.
The &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;paper&lt;/a&gt; presents two model sizes for BERT:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip&quot;&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/a&gt;: 24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip&quot;&gt;BERT-Large, Cased (Whole Word Masking)&lt;/a&gt;: 24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT is the best example of Transfer Learning where we train a general-purpose model on a large text corpus and use that model to solve different NLP tasks. BERT is the first unsupervised, deeply bidirectional system for pre-training NLP. Pre-trained models can be either context-free or contextual. Models like word2vec &amp;amp; GloVe are Context-free models. These models generate a single “word embedding” representation for each word in the vocabulary. But, based on the context in which a word is used it might have a different meaning &amp;amp; hence a different representation, i.e., more than one representation of same word is possible, which is handled by Contextual models.&lt;/p&gt;

&lt;p&gt;BERT is a contextual model &amp;amp; takes learnings from techniques like Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. Major enhancement comes with being &lt;em&gt;Deeply Bidirectional&lt;/em&gt;, which means, while computing the representation of a word, it takes into account both left &amp;amp; right context in a “deep” manner.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem statement&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given the document, anaphora, and two antecedents or precedents, resolve, which of the two antecedents/precedents does the anaphora refer to.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution approach&lt;/strong&gt;
Here, we try to use BERT to solve the above mentioned probelm. The problem is formulated as a binary classification problem with two antecedents/precedents being the two classes.&lt;/p&gt;

&lt;p&gt;Features used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Anaphora&lt;/li&gt;
  &lt;li&gt;First antecedent or precedent&lt;/li&gt;
  &lt;li&gt;Second antecedent or precedent&lt;/li&gt;
  &lt;li&gt;Preceeding n words from anaphora&lt;/li&gt;
  &lt;li&gt;Preceeding n words from first antecedent or precedent&lt;/li&gt;
  &lt;li&gt;Preceeding n words from second antecedent or precedent&lt;/li&gt;
  &lt;li&gt;Head word for anaphora&lt;/li&gt;
  &lt;li&gt;Head word for first antecedent or precedent&lt;/li&gt;
  &lt;li&gt;Head word for second antecedent or precedent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We run forward propogation through BERT on our data and extract the output of last layer. This output is the embeddings that we use for creating our features. Once all the feature words/expression are extracted, we extract BERT embeddings for these features. These embeddings are then concatenated &amp;amp; used as features to train the Multi-layer perceptron model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steps in detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To start with you must be having development &amp;amp; test sets. Development set will be used for training the model &amp;amp; test set will be used for final score.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create input dataset for BERT by setting “create_bert_input=True”
This step will create data in format neeeded by BERT &amp;amp; save it to a text file.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from coreference import CoreferenceResolution
CoreferenceResolution.create_bert_input(development_data, &apos;dev&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;input_&lt;data_type&gt;.txt file will be generated in temp folder of project&lt;/data_type&gt;&lt;/li&gt;
  &lt;li&gt;Use the input file to train BERT embeddings.&lt;/li&gt;
  &lt;li&gt;Do the above steps for all the datasets (train/test)&lt;/li&gt;
  &lt;li&gt;If you already have BERT embeddings, set create_bert_input=False &amp;amp; run the script.&lt;/li&gt;
  &lt;li&gt;Once you have the embeddings, extract relevant features. Currently we extract above mentioned features from the text. This step has scope for improvements.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;development_data[&apos;feature_words&apos;] = development_data.apply(CoreferenceResolution.extract_features, axis=1)
test_data[&apos;feature_words&apos;] = test_data.apply(CoreferenceResolution.extract_features, axis=1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Features that we have created are words/phrases, we need to convert them to numerical format using embeddings. So, extract embeddings for these features using:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;feature_em_dev   = CoreferenceResolution.extract_bert_embedding_for_word(development_data, &apos;dev&apos;)
feature_em_test  = CoreferenceResolution.extract_bert_embedding_for_word(test_data, &apos;test&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Concatenate embeddings to create features
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dev_emb_all = CoreferenceResolution.merge_all_features(development_emb, feature_em_dev)
test_emb_all = CoreferenceResolution.merge_all_features(test_emb, feature_em_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We can add additional features to the dataframe at this step.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Next we train the classifier using the above created features. We used multi-layered perceptrons for training the model. Again, this piece can be modified to try out other classifiers/networks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The piece of work described here is still under development and we are making improvements to achieve better accuracy. More of it will be described in part-2 of this series.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] https://nlp.stanford.edu/courses/cs224n/2003/fp/iqsayed/project_report.pdf&lt;/p&gt;

&lt;p&gt;[2] https://github.com/google-research/bert&lt;/p&gt;

&lt;p&gt;[3] https://www.kaggle.com/c/gendered-pronoun-resolution&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page&apos;s canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page&apos;s unique identifier variable
};
*/
(function() { // DON&apos;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&apos;script&apos;);
s.src = &apos;//agarnitin86-github-io.disqus.com/embed.js&apos;;
s.setAttribute(&apos;data-timestamp&apos;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;

</description>
        <pubDate>Sat, 23 Nov 2019 23:19:34 +0530</pubDate>
        <link>http://localhost:4000/blog/2019/11/23/anaphora-resolution</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2019/11/23/anaphora-resolution</guid>
        
        
        <category>NLP,</category>
        
        <category>Analphora</category>
        
        <category>Resolution</category>
        
        <category>BERT</category>
        
      </item>
    
      <item>
        <title>Tricks, Tips &amp; solutions</title>
        <description>&lt;h2 id=&quot;this-resource-contains-help-functionssteps-for-following-tasks&quot;&gt;This resource contains help functions/steps for following tasks:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Using ColumnTransformer &lt;a href=&quot;#col-trans&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Search for available GPUs&lt;/li&gt;
  &lt;li&gt;Steps to handle nan in loss during training&lt;/li&gt;
  &lt;li&gt;Steps to increase gpu utilization and speed of processing&lt;/li&gt;
  &lt;li&gt;Commands to monitor gpu usage&lt;/li&gt;
  &lt;li&gt;Commands to run tensorboard&lt;/li&gt;
  &lt;li&gt;Command to create tar file&lt;/li&gt;
  &lt;li&gt;Command to compute per user memory usage&lt;/li&gt;
  &lt;li&gt;Shell script to compress all files in a directory and delete the originial file from the directory. This script will create separate tar file for each file in the directory&lt;/li&gt;
  &lt;li&gt;Setting up tensorflow environment variables&lt;/li&gt;
  &lt;li&gt;Setting up LD_LIBRARY_PATH variable for tensorflow&lt;/li&gt;
  &lt;li&gt;Some useful steps to setup server for Deep Learning&lt;/li&gt;
  &lt;li&gt;Shell command to check folder wise disk usage&lt;/li&gt;
  &lt;li&gt;Filezilla: Error: Disconnected: No supported authentication methods available (server sent: publickey)&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;S.No.&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Category&lt;/th&gt;
      &lt;th&gt;Task&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Link&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Tensorflow&lt;/td&gt;
      &lt;td&gt;Tensorflow version compatibility chart&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;https://www.tensorflow.org/install/source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Cuda, Tensorflow&lt;/td&gt;
      &lt;td&gt;Steps needed to install cuda &amp;amp; runtime libraries&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;https://www.tensorflow.org/install/gpu#ubuntu_1604_cuda_10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Using ColumnTransformer&lt;/strong&gt;
This code demonstrates how to use ColumnTransformer&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.compose&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColumnTransformer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.compose&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_column_transformer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OneHotEncoder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;region&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;a&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;a&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;b&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;c&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;country&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;c1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;c1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;c2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;c3&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;categorical_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;region&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;country&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preprocessor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_column_transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OneHotEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;categorical_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Search for available GPUs&lt;/strong&gt;
How to get list of GPUs visible to tensorflow/keras&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from keras import backend
backend.tensorflow_backend._get_available_gpus()

import keras
import tensorflow as tf
config = tf.ConfigProto( device_count = {&apos;GPU&apos;: 1 , &apos;CPU&apos;: 56} ) 
sess = tf.Session(config=config) 
keras.backend.set_session(sess)

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

import tensorflow as tf
tf.test.is_gpu_available() tells if the gpu is available
tf.test.gpu_device_name returns the name of the gpu device

with tf.Session() as sess:
  devices = sess.list_devices()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Steps to handle nan in loss during training:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Normalize your outputs by quantile normalizing or z scoring. To be rigorous, compute this transformation on the training data, not on the entire dataset.&lt;/li&gt;
  &lt;li&gt;Add regularization, either by increasing the dropout rate or adding L1 and L2 penalties to the weights.&lt;/li&gt;
  &lt;li&gt;reduce the size of your network.&lt;/li&gt;
  &lt;li&gt;Increase the batch size from 32 to 128. 128 is fairly standard and could potentially increase the stability of the optimization.&lt;/li&gt;
  &lt;li&gt;In Keras you can use clipnorm=1 (see https://keras.io/optimizers/) to simply clip all gradients with a norm above 1.&lt;/li&gt;
  &lt;li&gt;It turns out, one of the images that I was handing to my CNN (and doing mean normalization on) was nothing but 0’s. I wasn’t checking for this case when I subtracted the mean and normalized by the std deviation and thus I ended up with an exemplar matrix which was nothing but nan’s.&lt;/li&gt;
  &lt;li&gt;The first thing you can try is changing your activation to LeakyReLU instead of using Relu or Tanh.&lt;/li&gt;
  &lt;li&gt;Column in my data set had all the same numerical value, making it effectively a worthless addition to the DNN.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Steps to increase gpu utilization and speed of processing&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Clear Keras session using K.clear_session&lt;/li&gt;
  &lt;li&gt;Use fit_gen instead of fit if data is huge&lt;/li&gt;
  &lt;li&gt;Use early stopping&lt;/li&gt;
  &lt;li&gt;Use multiple workers in fit/fit_gen. (This has no been checked)&lt;/li&gt;
  &lt;li&gt;Try increasing validation_freq&lt;/li&gt;
  &lt;li&gt;If using LSTM try using cuDNNLSTM instead of LSTM&lt;/li&gt;
  &lt;li&gt;If using LSTM try using unroll = True option&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Commands to monitor gpu usage&lt;/strong&gt;
$ nvidia-smi -l 1 
$ watch -n 0.5 nvidia-smi&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Commands to run tensorboard&lt;/strong&gt; 
tensorboard –logdir=./iteration_7/model/logs/ –port=8097 –host=127.0.0.1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Command to create tar file&lt;/strong&gt; 
tar -czvf moscow.tar.gz ./store_log_calendar_Moscow.csv&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Command to compute per user memory usage&lt;/strong&gt; 
ps aux | awk ‘{arr[$1]+=$4}; END {for (i in arr) {print i,arr[i]}}’ | sort -k2&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Shell script to compress all files in a directory and delete the originial file from the directory. This script will create separate tar file for each file in the directory&lt;/strong&gt; 
for i in * ; do tar cvzf $i.tar.gz $i; rm -rf $i; done&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;setting-up-tensorflow-environment&quot;&gt;Setting up tensorflow environment&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Environment variables for tensorflow-gpu:&lt;/strong&gt;
TF_FORCE_GPU_ALLOW_GROWTH = True&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration parameters to be set in code&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config.gpu_options.allow_growth = True
config.gpu_options.visible_device_list = which_gpu
config.gpu_options.per_process_gpu_memory_fraction = 0.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Code to dynamically grow GPU memory (instead of allocating everything at once)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow as tf 
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU
sess = tf.Session(config=config)
set_session(sess) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Some useful steps to setup server for Deep Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Download the anaconda installer&lt;/strong&gt;
$ wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh
or
$ wget https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Run the installer&lt;/strong&gt;
$ bash Anaconda3-2019.03-Linux-x86_64.sh
$ source .bashrc&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upgrade conda (Might not be required, check the instructions)&lt;/strong&gt;
conda upgrade conda
or
conda upgrade –all&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create new virtual environment in conda&lt;/strong&gt;
$ conda create -n venv_rb python=3.5
$ conda activate venv_rb&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some useful commands to check the environment&lt;/strong&gt;
$ conda info
$ conda list
$ which pip&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Installing some useful packages&lt;/strong&gt;
$ pip install –upgrade tensorflow-gpu
$ pip install keras
$ conda install pandas
$ pip install matplotlib
$ pip install -U scikit-learn
$ pip install jupyter
$ pip install pandas-profiling&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setting up LD_LIBRARY_PATH variable for tensorflow&lt;/strong&gt;
export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additional steps needed to install cuda &amp;amp; runtime libraries (Select based on ubuntu &amp;amp; tensorflow-gpu versions)&lt;/strong&gt;
https://www.tensorflow.org/install/gpu#ubuntu_1604_cuda_10&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tensorflow version compatibility chart&lt;/strong&gt;
https://www.tensorflow.org/install/source&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Shell command to check folder wise disk usage&lt;/strong&gt;
$ du -h –max-depth=1 /data/rb/users/|sort -h&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Filezilla error while connecting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Error:	Disconnected: No supported authentication methods available (server sent: publickey)
Error:	Could not connect to server&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;
If you have private key file (.ppk file):
In Filezilla,
Go to Edit » Settings » SFTP » Add Key File
Upload your private key file and try to re-connect.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;How to add equations to you Readme file in github&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;
https://stackoverflow.com/questions/35498525/latex-rendering-in-readme-md-on-github
Take your latex equation and go to http://www.codecogs.com/latex/eqneditor.php, at the bottom of the area where your equation appears displayed there is a tiny dropdown menu, pick URL encoded and then paste that in your github markdown&lt;/p&gt;

&lt;hr /&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page&apos;s canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page&apos;s unique identifier variable
};
*/
(function() { // DON&apos;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&apos;script&apos;);
s.src = &apos;//agarnitin86-github-io.disqus.com/embed.js&apos;;
s.setAttribute(&apos;data-timestamp&apos;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
</description>
        <pubDate>Tue, 19 Nov 2019 23:19:34 +0530</pubDate>
        <link>http://localhost:4000/blog/2019/11/19/Help</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2019/11/19/Help</guid>
        
        
        <category>python,</category>
        
        <category>sklearn,</category>
        
        <category>gpu</category>
        
        <category>Generic</category>
        
      </item>
    
      <item>
        <title>Text classification using CNN : Example</title>
        <description>&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;

&lt;p&gt;This blog is inspired from the &lt;a href=&quot;http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/&quot;&gt;wildml blog&lt;/a&gt; on text classification using convolution neural networks. This blog is based on the tensorflow code given in wildml blog. Here, in this blog i have taken two senetences as example and tried to explain what happens to the input data at each layer of the CNN. For each layer of the network it explains input for the layer, processing done and the output of each layer along with the shape of input and output data.&lt;br /&gt;
This blog does not explains the basic working of CNN. It assumes that the reader has some understanding of CNN’s&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py&quot;&gt;Github link to the code&lt;/a&gt; being explained.&lt;/p&gt;

&lt;p&gt;I will try to explain the network in the same order as it is in code.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data processing&lt;/li&gt;
  &lt;li&gt;Embedding Layer&lt;/li&gt;
  &lt;li&gt;Convolution Layer&lt;/li&gt;
  &lt;li&gt;Pooling Layer&lt;/li&gt;
  &lt;li&gt;Dropout Layer&lt;/li&gt;
  &lt;li&gt;Output Layer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I Will be using following two sentences as input for our classification task:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sentence 1&lt;/strong&gt; : The camera quality is very good&lt;br /&gt;
&lt;strong&gt;Sentence 2&lt;/strong&gt; : The battery life is good&lt;/p&gt;

&lt;p&gt;Here we have two sentences of length 6 and 5 respectively.&lt;br /&gt;
We also assume that their are two classes for our classification model, Positive and Negative.&lt;/p&gt;

&lt;h3 id=&quot;data-preprocessing-and-building-the-vocabulary&quot;&gt;Data preprocessing and building the vocabulary&lt;/h3&gt;

&lt;p&gt;Since the sentences are of different length, we pad our sentences with special &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;PAD&amp;gt;&lt;/code&gt; token to make the lengths of the two sentences equal.&lt;/p&gt;

&lt;p&gt;So now we have our sentences modified as :&lt;br /&gt;
&lt;strong&gt;Sentence 1&lt;/strong&gt; : the camera quality is very good&lt;br /&gt;
&lt;strong&gt;Sentence 2&lt;/strong&gt; : the battery life is good &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;PAD&amp;gt;&lt;/code&gt;.&lt;br /&gt;
Now, both the sentences are of same length. We proceed to build the vocabulary index.&lt;br /&gt;
&lt;strong&gt;Vocabulary index&lt;/strong&gt; is a mapping of integer to each unique word in the corpus.&lt;br /&gt;
In our case, size of vocabulary index will be 9, since there are 9 unique tokens. Vocabulary is as follows
&lt;img src=&quot;/images/vocabulary.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 500px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Corresponding code from the blog&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;vocab_processor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VocabularyProcessor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_document_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In tensorflow, tensorflow.contrib.learn.preprocessing.VocabularyProcessor is used for building the vocabulary.&lt;br /&gt;
Use &lt;a href=&quot;http://stackoverflow.com/questions/40661684/tensorflow-vocabularyprocessor&quot;&gt;this link&lt;/a&gt; to see how to extract vocabulary from the vocab_processor object.&lt;/p&gt;

&lt;p&gt;Next, each sentence is converted into vector of integers.&lt;br /&gt;
&lt;strong&gt;Sentence 1&lt;/strong&gt; : [1, 2, 3, 4, 5, 6]&lt;br /&gt;
&lt;strong&gt;Sentence 2&lt;/strong&gt; : [1, 7, 8, 4, 6, 0]&lt;/p&gt;

&lt;h3 id=&quot;embedding-layer&quot;&gt;Embedding Layer&lt;/h3&gt;

&lt;h4 id=&quot;configurable-parameters-for-embedding-layer&quot;&gt;Configurable parameters for embedding layer&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;batch_size&lt;/strong&gt; = 2 (Since we have only two sentences here)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;sequence length&lt;/strong&gt; = 6 (Max length of the senteces)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;num_classes&lt;/strong&gt;       = 2 (Number of output classes. Positive and negative in our case)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;vocabulary size (V)&lt;/strong&gt; = 9&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;trainable-parameters-for-embedding-layer&quot;&gt;Trainable parameters for embedding layer&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Embedding vector size (E)&lt;/strong&gt; = 128 (Size of the embedding vector)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Embedding matrix (W) of shape&lt;/strong&gt; = [V * E] = [9 * 128]&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;input&quot;&gt;Input&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;shape of input (input_x)&lt;/strong&gt; = [batch_size, sequence_length] = [2, 6]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;shape of input (input_y)&lt;/strong&gt; = [batch_size, num_classes] = [2, 2]&lt;br /&gt;
Here, input_y are the output labels of input sentences encoded using one-hot encoding. Assuming both the sentences are positive (which will not be the actual case. There will also be negative sentences.)&lt;br /&gt;
input_y = [ [1, 0], [1,0] ]&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;working-of-embedding-layer&quot;&gt;Working of embedding layer&lt;/h4&gt;

&lt;p&gt;As per the code in the &lt;a href=&quot;http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/&quot;&gt;wildml blog&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Input to the embedding layer is a tuple whose length is equal to the batch size, i.e, number of sentences in a batch.&lt;br /&gt;
Each element of the tuple is an array of numbers, representing sentence in the form of indexes into vocabulary.&lt;br /&gt;
For example : &lt;br /&gt;
&lt;img src=&quot;/images/SentenceRepresentation.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 500px;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;embedding-lookup&quot;&gt;Embedding Lookup&lt;/h4&gt;
&lt;p&gt;Statement from the &lt;a href=&quot;http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/&quot;&gt;wildml blog&lt;/a&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedded_chars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This statement takes as input, input_x from the previous step.&lt;br /&gt;
For each sentence, and for each word, it looks up (indexes) the embedding matrix (which is 9 * 128 in our case) (w) and returns the corresponding embedding vector for each word of each sentence.&lt;br /&gt;
When i printed the variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;embedded_chars&lt;/code&gt;, it is actually a list structured as follows :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;embedded_chars is a list of size 1,&lt;/li&gt;
  &lt;li&gt;its 0th element is a list of size batch_size, i.e, number of sentences, (2 for this ex)&lt;/li&gt;
  &lt;li&gt;each element of this list is a list of size sequence_length, (6 for this ex)&lt;/li&gt;
  &lt;li&gt;and, each element of this list is a list of size embedding vector (128 for this ex.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, the shape of tensor is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size, sequence_length, embedding_vector_length]&lt;/code&gt;, i.e, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 6 * 128]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Each element of the word vector is a real value. In the next satement of the blog, i.e,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedded_chars_expanded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedded_chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;tensor is reshaped to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size, sequence_length, embedding_vector_length, 1]&lt;/code&gt;,&lt;br /&gt;
i.e, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 6 * 128 * 1]&lt;/code&gt;. So that, each element of the word vector is itself a list of size 1, instead of a real number.&lt;br /&gt;
Both the above statements can be easily understood from the following figure, where out = embedded_chars&lt;br /&gt;
&lt;img src=&quot;/images/SentenceRepresentationWordVector.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 500px;&quot; /&gt;&lt;br /&gt;
This completes the explanation of embedding layer&lt;/p&gt;

&lt;h4 id=&quot;output-of-embedding-layer-&quot;&gt;Output of embedding layer :&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Tensor of shape : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size, sequence_length, embedding_vector_length, 1]&lt;/code&gt;, i.e, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 6 * 128 * 1]&lt;/code&gt;.
Next layer in the model is the convolution layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolution-layer&quot;&gt;Convolution Layer&lt;/h3&gt;

&lt;h4 id=&quot;configurable-parameters-of-convolution-layer&quot;&gt;Configurable parameters of convolution layer&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;filter_sizes&lt;/strong&gt; : 3, 4, 5&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;num_filters&lt;/strong&gt;: 128&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stride length&lt;/strong&gt; : [1 * 1 * 1 * 1]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Padding&lt;/strong&gt; : VALID&lt;br /&gt;
As oppossed to 2D filters in images, here in text classification we use 1D filters. We will be using filters of sizes 3,4,5. First we will go through the process of a single filter. Same will work for the other two sizes. Later we will see, how to merge the output from all the three filter sizes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;trainable-parameters-of-convolution-layer&quot;&gt;Trainable parameters of convolution layer&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Weight matrix (W)&lt;/strong&gt; : its shape is same as shape of filter.(Discussed next)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bias(b)&lt;/strong&gt; : = [0.1, 0.1, 0.1……..0.1] Since there are 128 filters, there will be 128 bias values.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;input-1&quot;&gt;Input&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Output of the expanded embedding lookup step. Its shape is&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size, sequence_length, embedding_vector_length, 1]&lt;/code&gt;, i.e, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 6 * 128 * 1]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Now, what is the shape of filter ?&lt;/strong&gt;
Lets take filter of size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filter_size = 3&lt;/code&gt;. In this case, shape of the filter is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[filter_height, fiter_width]&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[filter_size, embedding_vector_length]&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[3 * 128]&lt;/code&gt;. This means that the filter sees 3 words (or embedding vectors) at  a time, and based on the stride keeps seeing the 3 word vectors.&lt;br /&gt;
We have only one input channel, therefore, num_input_channels = 1&lt;br /&gt;
Number of output channels or Number of filters is 128.
Therefore, shape of the filter tensor becomes : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[filter_size, embedding_vector_len, num_input_channels, num_filters] = [ 3 * 128 * 1 * 128]&lt;/code&gt;
The following figure shows the mapping between input matrix and filter of size 3 : 
&lt;img src=&quot;/images/CNN_Mapping_1.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 500px;&quot; /&gt;&lt;br /&gt;
Here, the input only represents one sentence NOT both.&lt;br /&gt;
I will explain how to apply convolution filter on the input in the next step, but, before that let us see how to compute the output shape for this filter of size 3.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now, what is the shape of output filter ?&lt;/strong&gt;&lt;br /&gt;
For 1D case, as in text,&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;out_rows = (W1- F + 2 * P) / S + 1&lt;/code&gt;&lt;br /&gt;
where,&lt;br /&gt;
W1 = sequence_length (Number of input words) = 6 (in our case)&lt;br /&gt;
F = height of filter or filter size = 3 (in our case)&lt;br /&gt;
P = padding = 0 (in our case)&lt;br /&gt;
Stride = 1 (in our case)&lt;/p&gt;

&lt;p&gt;Therefore,&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;out_rows = (6 - 3 + 2 * 0) / 1 + 1 = 4&lt;/code&gt;&lt;br /&gt;
Since the filter moves only in 1D, shape of output = 4 * 1, for a single sentence with a fiter_size = 3 and for one output channel.&lt;br /&gt;
Now, since we have batch of sentences and num_filters = 128, we get 4 * 1 outputs for each combination. Therefore, shape of output tensor for fiter_size (3) is&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size * 4 * 1 * num_filters] = [2 * 4 * 1 * 128]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Similarly, we can compute the output shapes for filters of size 4 and 5 as follows : &lt;br /&gt;
&lt;strong&gt;For  filter_size (4)&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(6 - 4 + 2 * 0) / 1 + 1 = 3&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 3 * 1 * 128]&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;For  filter_size (5)&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(6 - 5 + 2 * 0) / 1 + 1 = 2&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 2 * 1 * 128]&lt;/code&gt;&lt;br /&gt;
Now let us see a simple calculation showing how filter is applied to input and what is the role of weight matrix and bias.&lt;br /&gt;
Corresponding code statement:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedded_chars_expanded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;VALID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;conv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/CNN_Mapping_WithCalculation.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 500px;&quot; /&gt;&lt;br /&gt;
From the above figure, we get the result of applying fiter to patch of size 3 * 128, which is 5.76.&lt;br /&gt;
In the case of tensorflow conv2d function, the conv2d function only performs elementwise multiplication of input patch and filter. The bias needs to be added separately.&lt;br /&gt;
This is done in the next line of the code where nonlinearity is being applied along with bias addition.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here, bias is being added to the output of conv layer for each patch-filter convolution. So, for the current patch we get 5.76 + 0.1 = 5.86.
On this output, relu is applied to, i.e, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max(0, x) = max(0, 5.86)  = 5.86&lt;/code&gt;
So, we get h = 5.86 for the current patch.
The figure below describes the process for all the filters i.e, num_filters = 128.&lt;br /&gt;
&lt;img src=&quot;/images/FlowdiagCNN.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 800px;&quot; /&gt;&lt;br /&gt;
The figure above shows the output, for, when 128 filters of size 3 are applied on  a single sentence. When applied to batch, shape becomes [batch_size * 4 * 1 * 128] = [2 * 4 * 1 * 128]&lt;/p&gt;

&lt;h3 id=&quot;pooling-layer&quot;&gt;Pooling Layer&lt;/h3&gt;

&lt;h4 id=&quot;configurable-parameters-of-pooling-layer&quot;&gt;Configurable parameters of pooling layer&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ksize&lt;/strong&gt; : shape of the pool operator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1 * 4 * 1 * 128]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;strides&lt;/strong&gt; : same as conv2d &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1 * 1 * 1 * 1]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Their are no training parameters for this layer. As only max operation is performed.&lt;br /&gt;
Corresponding code statement:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pooled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ksize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filter_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;VALID&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pool&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;input-2&quot;&gt;Input&lt;/h4&gt;
&lt;p&gt;Output of the convolution layer, whose shape is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size * 4 * 1 * num_filters ] = [2 * 4 * 1 * 128]&lt;/code&gt;. Here, we are using the output from the filter of size, filter_size = 3&lt;br /&gt;
In the max pooling layer we perform max operation on the output of each filter over the sentence, i.e, max operation on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4 * 1&lt;/code&gt; output of each filter.
The max  can be selected from all the 4 values, or we can define a different ksize. &lt;br /&gt;
Here, we are using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ksize = [1 * (4 * 1) * 128]&lt;/code&gt;, which means select the max out of all the four values. Since the selection is done from all the values, we get a single value as output for each of the 128 filters. This is demonstrated in the figure below :&lt;br /&gt;
&lt;img src=&quot;/images/PoolingLayer_1.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, we have max pooling output shape = [1 * 1 * 128] (128 - for each filter)&lt;/p&gt;

&lt;p&gt;Now, suppose we change the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ksize = [1 * 2 * 1 * 128]&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stride = [1 * 1 * 1 * 1]&lt;/code&gt;, we get output of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shape [3 * 1]&lt;/code&gt; for each of 128 filters as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/PoolingLayer_2.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In our text classification problem filter moves only in one direction, therefore, size = 3 * 1. It it had moved along horizontal direction also (as in images), the shape of output would have been (3 * a) where a &amp;gt; 1&lt;/p&gt;

&lt;h4 id=&quot;merging-the-output-of-max-pooling-layer-for-each-filter-size3-4-5&quot;&gt;Merging the output of max pooling layer for each filter size(3, 4, 5).&lt;/h4&gt;
&lt;p&gt;Corresponding code statement:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_pool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pooled_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, for the current example we have 3 different filter size (3,4,5). For each filter size we get a tensor of shape [1 * 1 * 128]. These tensors are concatenated. So, we have 3 tensors of shape [1 * 1 * 128] to get a tensor of shape [3 * 1 * 1 * 128]. Since, the same is done for each sentence in the batch, shape of the tensor becomes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size * 3 * 1 * 1 * 128] = [2 * 3 * 1 * 1 * 128]&lt;/code&gt; as shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ConcatenateReshape.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 800px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, in the code,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_pool_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_filters_total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;a reshape is performed to get a tensor of shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size, (3* 128)] = [batch_size , 384]  = [2 * 384]&lt;/code&gt;. This means, now for each sentence we have a fiter output in a row as shown above.&lt;/p&gt;

&lt;h3 id=&quot;dropout-layer&quot;&gt;Dropout Layer&lt;/h3&gt;
&lt;p&gt;Corresponding code statement&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_drop&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_pool_flat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_keep_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this layer simply dropout is applied. Input and output shapes remain the same.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input shape    : [batch_size * 384] = [2 * 384]&lt;/li&gt;
  &lt;li&gt;Output shape : [batch_size * 384] = [2 * 384]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;output-layer&quot;&gt;Output Layer&lt;/h3&gt;
&lt;p&gt;Corresponding code statement&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xw_plus_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;scores&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;configurable-parameters-of-output-layer&quot;&gt;Configurable parameters of output layer&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Weight matrix (W)&lt;/strong&gt; of shape [ total_num_filters * num_classes]&lt;br /&gt;
where, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;total_num_filters = length([3, 4, 5]) * num_filters = 3 * 128 = 384&lt;/code&gt;,&lt;br /&gt;
          &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_classes = 2&lt;/code&gt;,
Therefore, shape of W = [384 * 2]&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bias (b)&lt;/strong&gt; of shape [num_classes] = [2] or&lt;br /&gt;
b = [0.1, 0.1]&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;input-3&quot;&gt;Input&lt;/h4&gt;
&lt;p&gt;Output of the previous dropout layer with shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[2 * 384]&lt;/code&gt;. So, the size of each input vector is 384.&lt;/p&gt;

&lt;h4 id=&quot;working&quot;&gt;Working&lt;/h4&gt;
&lt;p&gt;For each input vector (x) from the previous layer, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xW + b&lt;/code&gt; is calculated,&lt;br /&gt;
where x is a row vector of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[384]&lt;/code&gt; elements, W is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[384 * 2]&lt;/code&gt;.&lt;br /&gt;
So, for each sentence we get a vector of length 2 (num_classes),&lt;br /&gt;
and, for the batch of size batch_size, output shape is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[batch_size * num_classes] = [2 * 2]&lt;/code&gt; as shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/OutputLayer.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page&apos;s canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page&apos;s unique identifier variable
};
*/
(function() { // DON&apos;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&apos;script&apos;);
s.src = &apos;//agarnitin86-github-io.disqus.com/embed.js&apos;;
s.setAttribute(&apos;data-timestamp&apos;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;

</description>
        <pubDate>Fri, 23 Dec 2016 23:19:34 +0530</pubDate>
        <link>http://localhost:4000/blog/2016/12/23/text-classification-cnn</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2016/12/23/text-classification-cnn</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
        <category>NLP</category>
        
      </item>
    
  </channel>
</rss>
